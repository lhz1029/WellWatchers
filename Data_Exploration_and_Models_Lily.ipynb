{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Status of Wells Across Tanzania\n",
    "\n",
    "Exploration done by WellWatchers Avi Saraf, Lily Zhang, and Cindy Zhao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_values = pd.read_csv(\"clean_training_set_values.csv\")\n",
    "df_labels = pd.read_csv(\"training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the features (i.e. values) and the labels into one DataFrame\n",
    "df = pd.merge(df_values, df_labels, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>funder</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>installer</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>basin</th>\n",
       "      <th>subvillage</th>\n",
       "      <th>region</th>\n",
       "      <th>...</th>\n",
       "      <th>quantity</th>\n",
       "      <th>quantity_group</th>\n",
       "      <th>source</th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_class</th>\n",
       "      <th>waterpoint_type</th>\n",
       "      <th>waterpoint_type_group</th>\n",
       "      <th>date_recorded_offset_days</th>\n",
       "      <th>date_recorded_month</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69572</td>\n",
       "      <td>6000</td>\n",
       "      <td>Other</td>\n",
       "      <td>1390</td>\n",
       "      <td>Other</td>\n",
       "      <td>34.938093</td>\n",
       "      <td>-9.856322</td>\n",
       "      <td>Lake Nyasa</td>\n",
       "      <td>Mnyusi B</td>\n",
       "      <td>Iringa</td>\n",
       "      <td>...</td>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>1024</td>\n",
       "      <td>Mar</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8776</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>1399</td>\n",
       "      <td>Other</td>\n",
       "      <td>34.698766</td>\n",
       "      <td>-2.147466</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>Nyamara</td>\n",
       "      <td>Mara</td>\n",
       "      <td>...</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>301</td>\n",
       "      <td>Mar</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34310</td>\n",
       "      <td>25</td>\n",
       "      <td>Other</td>\n",
       "      <td>686</td>\n",
       "      <td>Other</td>\n",
       "      <td>37.460664</td>\n",
       "      <td>-3.821329</td>\n",
       "      <td>Pangani</td>\n",
       "      <td>Majengo</td>\n",
       "      <td>Manyara</td>\n",
       "      <td>...</td>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>dam</td>\n",
       "      <td>dam</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe multiple</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>310</td>\n",
       "      <td>Feb</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67743</td>\n",
       "      <td>0</td>\n",
       "      <td>Unicef</td>\n",
       "      <td>263</td>\n",
       "      <td>Other</td>\n",
       "      <td>38.486161</td>\n",
       "      <td>-11.155298</td>\n",
       "      <td>Ruvuma / Southern Coast</td>\n",
       "      <td>Mahakamani</td>\n",
       "      <td>Mtwara</td>\n",
       "      <td>...</td>\n",
       "      <td>dry</td>\n",
       "      <td>dry</td>\n",
       "      <td>machine dbh</td>\n",
       "      <td>borehole</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe multiple</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>338</td>\n",
       "      <td>Jan</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19728</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>31.130847</td>\n",
       "      <td>-1.825359</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>Kyanyamisa</td>\n",
       "      <td>Kagera</td>\n",
       "      <td>...</td>\n",
       "      <td>seasonal</td>\n",
       "      <td>seasonal</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>903</td>\n",
       "      <td>Jul</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  amount_tsh  funder  gps_height installer  longitude   latitude  \\\n",
       "0  69572        6000   Other        1390     Other  34.938093  -9.856322   \n",
       "1   8776           0   Other        1399     Other  34.698766  -2.147466   \n",
       "2  34310          25   Other         686     Other  37.460664  -3.821329   \n",
       "3  67743           0  Unicef         263     Other  38.486161 -11.155298   \n",
       "4  19728           0   Other           0     Other  31.130847  -1.825359   \n",
       "\n",
       "                     basin  subvillage   region       ...            quantity  \\\n",
       "0               Lake Nyasa    Mnyusi B   Iringa       ...              enough   \n",
       "1            Lake Victoria     Nyamara     Mara       ...        insufficient   \n",
       "2                  Pangani     Majengo  Manyara       ...              enough   \n",
       "3  Ruvuma / Southern Coast  Mahakamani   Mtwara       ...                 dry   \n",
       "4            Lake Victoria  Kyanyamisa   Kagera       ...            seasonal   \n",
       "\n",
       "  quantity_group                source           source_type source_class  \\\n",
       "0         enough                spring                spring  groundwater   \n",
       "1   insufficient  rainwater harvesting  rainwater harvesting      surface   \n",
       "2         enough                   dam                   dam      surface   \n",
       "3            dry           machine dbh              borehole  groundwater   \n",
       "4       seasonal  rainwater harvesting  rainwater harvesting      surface   \n",
       "\n",
       "               waterpoint_type waterpoint_type_group  \\\n",
       "0           communal standpipe    communal standpipe   \n",
       "1           communal standpipe    communal standpipe   \n",
       "2  communal standpipe multiple    communal standpipe   \n",
       "3  communal standpipe multiple    communal standpipe   \n",
       "4           communal standpipe    communal standpipe   \n",
       "\n",
       "  date_recorded_offset_days date_recorded_month    status_group  \n",
       "0                      1024                 Mar      functional  \n",
       "1                       301                 Mar      functional  \n",
       "2                       310                 Feb      functional  \n",
       "3                       338                 Jan  non functional  \n",
       "4                       903                 Jul      functional  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploratory analysis\n",
    "\n",
    "##Categorical Features\n",
    "\n",
    "We have the following categorical variables:\n",
    "- funder\n",
    "- installer\n",
    "- basin\n",
    "- subvillage\n",
    "- region\n",
    "- scheme_management\n",
    "- management\n",
    "- management_group\n",
    "- public_meeting (T/F)\n",
    "- permit (T/F)\n",
    "- extraction_type\n",
    "- extraction_type_group\n",
    "- extraction_type_class\n",
    "- payment\n",
    "- payment_type\n",
    "- water_quality\n",
    "- quality_group\n",
    "- waterpoint_type\n",
    "- waterpoint_type_group\n",
    "\n",
    "Here we see the unique values under the columns with categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Redundancies\n",
    "We see that some of the columns describe the same feature but have slightly different sets of values. This is particularly the case with features that also have a corresponding '_ _group_' column as well. For now, we will stick with the non-gropu features because they are more specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gravity' 'submersible' 'swn 80' 'nira/tanira' 'india mark ii' 'other'\n",
      " 'ksb' 'mono' 'windmill' 'afridev' 'other - rope pump' 'india mark iii'\n",
      " 'other - swn 81' 'other - play pump' 'cemo' 'climax' 'walimi']\n",
      "['gravity' 'submersible' 'swn 80' 'nira/tanira' 'india mark ii' 'other'\n",
      " 'mono' 'wind-powered' 'afridev' 'rope pump' 'india mark iii'\n",
      " 'other handpump' 'other motorpump']\n",
      "['gravity' 'submersible' 'handpump' 'other' 'motorpump' 'wind-powered'\n",
      " 'rope pump']\n"
     ]
    }
   ],
   "source": [
    "print pd.Series(df.extraction_type.ravel()).unique()\n",
    "#less specific\n",
    "print pd.Series(df.extraction_type_group.ravel()).unique()\n",
    "print pd.Series(df.extraction_type_class.ravel()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pay annually' 'never pay' 'pay per bucket' 'unknown'\n",
      " 'pay when scheme fails' 'other' 'pay monthly']\n",
      "['annually' 'never pay' 'per bucket' 'unknown' 'on failure' 'other'\n",
      " 'monthly']\n"
     ]
    }
   ],
   "source": [
    "#equivalent\n",
    "print pd.Series(df.payment.ravel()).unique()\n",
    "print pd.Series(df.payment_type.ravel()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communal standpipe' 'communal standpipe multiple' 'hand pump' 'other'\n",
      " 'improved spring' 'cattle trough' 'dam']\n",
      "['communal standpipe' 'hand pump' 'other' 'improved spring' 'cattle trough'\n",
      " 'dam']\n"
     ]
    }
   ],
   "source": [
    "#the difference here is the addition of 'communal standpipe multiple' in waterpoint_type\n",
    "print pd.Series(df.waterpoint_type.ravel()).unique()\n",
    "#less specific\n",
    "print pd.Series(df.waterpoint_type_group.ravel()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soft' 'salty' 'milky' 'unknown' 'fluoride' 'coloured' 'salty abandoned'\n",
      " 'fluoride abandoned']\n",
      "['good' 'salty' 'milky' 'unknown' 'fluoride' 'colored']\n"
     ]
    }
   ],
   "source": [
    "print pd.Series(df.water_quality.ravel()).unique()\n",
    "#less specific\n",
    "print pd.Series(df.quality_group.ravel()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spring' 'rainwater harvesting' 'dam' 'borehole' 'other' 'shallow well'\n",
      " 'river/lake']\n",
      "['spring' 'rainwater harvesting' 'dam' 'machine dbh' 'other' 'shallow well'\n",
      " 'river' 'hand dtw' 'lake' 'unknown']\n",
      "['groundwater' 'surface' 'unknown']\n"
     ]
    }
   ],
   "source": [
    "#less specific\n",
    "#'borehole' includes 'machine dbh' and 'hand dtw'\n",
    "#'river/lake' includes 'river' and 'lake'\n",
    "#'other' includes 'other' and 'unknown'\n",
    "print pd.Series(df.source_type.ravel()).unique()\n",
    "print pd.Series(df.source.values.ravel()).unique()\n",
    "#source and source_type are nested within source_class\n",
    "print pd.Series(df.source_class.values.ravel()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vwc' 'wug' 'other' 'private operator' 'water board' 'wua' 'company'\n",
      " 'water authority' 'parastatal' 'unknown' 'other - school' 'trust']\n",
      "['user-group' 'other' 'commercial' 'parastatal' 'unknown']\n"
     ]
    }
   ],
   "source": [
    "#management is nested within management_group\n",
    "print pd.Series(df.management.ravel()).unique()\n",
    "print pd.Series(df.management_group.ravel()).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of these features are redundant, so we delete one of the columns of these repeats. Regarding the features/feature_group label pairs, there is a nesting structure but it is so slight that we just delete one of the columns; otherwise, we would most likely run into problems of collinearity. We have two new dataframes, one with only the feature_group labels (less specific), and one with only the feature labels (more specific). We will try our regression on the less specific df_new first but keep df_new1 just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new1 = df.drop(['quantity_group','extraction_type_group','waterpoint_type_group','quality_group','source_type','payment', 'construction_year','latitude','longitude', 'subvillage','id'], axis=1)\n",
    "df_new = df.drop(['quantity_group','extraction_type','waterpoint_type','quality_group','source','payment', 'construction_year','latitude','longitude','subvillage','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>funder</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>installer</th>\n",
       "      <th>basin</th>\n",
       "      <th>region</th>\n",
       "      <th>population</th>\n",
       "      <th>public_meeting</th>\n",
       "      <th>scheme_management</th>\n",
       "      <th>permit</th>\n",
       "      <th>...</th>\n",
       "      <th>management_group</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>water_quality</th>\n",
       "      <th>quantity</th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_class</th>\n",
       "      <th>waterpoint_type_group</th>\n",
       "      <th>date_recorded_offset_days</th>\n",
       "      <th>date_recorded_month</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6000</td>\n",
       "      <td>Other</td>\n",
       "      <td>1390</td>\n",
       "      <td>Other</td>\n",
       "      <td>Lake Nyasa</td>\n",
       "      <td>Iringa</td>\n",
       "      <td>109</td>\n",
       "      <td>True</td>\n",
       "      <td>VWC</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>user-group</td>\n",
       "      <td>annually</td>\n",
       "      <td>soft</td>\n",
       "      <td>enough</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>1024</td>\n",
       "      <td>Mar</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>1399</td>\n",
       "      <td>Other</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>Mara</td>\n",
       "      <td>280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>user-group</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>301</td>\n",
       "      <td>Mar</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Other</td>\n",
       "      <td>686</td>\n",
       "      <td>Other</td>\n",
       "      <td>Pangani</td>\n",
       "      <td>Manyara</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "      <td>VWC</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>user-group</td>\n",
       "      <td>per bucket</td>\n",
       "      <td>soft</td>\n",
       "      <td>enough</td>\n",
       "      <td>dam</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>310</td>\n",
       "      <td>Feb</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Unicef</td>\n",
       "      <td>263</td>\n",
       "      <td>Other</td>\n",
       "      <td>Ruvuma / Southern Coast</td>\n",
       "      <td>Mtwara</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>VWC</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>user-group</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>dry</td>\n",
       "      <td>borehole</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>338</td>\n",
       "      <td>Jan</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>Kagera</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>other</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>seasonal</td>\n",
       "      <td>rainwater harvesting</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>903</td>\n",
       "      <td>Jul</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount_tsh  funder  gps_height installer                    basin   region  \\\n",
       "0        6000   Other        1390     Other               Lake Nyasa   Iringa   \n",
       "1           0   Other        1399     Other            Lake Victoria     Mara   \n",
       "2          25   Other         686     Other                  Pangani  Manyara   \n",
       "3           0  Unicef         263     Other  Ruvuma / Southern Coast   Mtwara   \n",
       "4           0   Other           0     Other            Lake Victoria   Kagera   \n",
       "\n",
       "   population public_meeting scheme_management permit       ...        \\\n",
       "0         109           True               VWC  False       ...         \n",
       "1         280            NaN             Other   True       ...         \n",
       "2         250           True               VWC   True       ...         \n",
       "3          58           True               VWC   True       ...         \n",
       "4           0           True               NaN   True       ...         \n",
       "\n",
       "  management_group payment_type water_quality      quantity  \\\n",
       "0       user-group     annually          soft        enough   \n",
       "1       user-group    never pay          soft  insufficient   \n",
       "2       user-group   per bucket          soft        enough   \n",
       "3       user-group    never pay          soft           dry   \n",
       "4            other    never pay          soft      seasonal   \n",
       "\n",
       "            source_type source_class waterpoint_type_group  \\\n",
       "0                spring  groundwater    communal standpipe   \n",
       "1  rainwater harvesting      surface    communal standpipe   \n",
       "2                   dam      surface    communal standpipe   \n",
       "3              borehole  groundwater    communal standpipe   \n",
       "4  rainwater harvesting      surface    communal standpipe   \n",
       "\n",
       "  date_recorded_offset_days date_recorded_month    status_group  \n",
       "0                      1024                 Mar      functional  \n",
       "1                       301                 Mar      functional  \n",
       "2                       310                 Feb      functional  \n",
       "3                       338                 Jan  non functional  \n",
       "4                       903                 Jul      functional  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Nestings\n",
    "\n",
    "In df_new, we know that:\n",
    "- management is nested within management_group\n",
    "- source is nested within source_class\n",
    "- extraction is nested within extraction_type_class\n",
    "- subvillage is nested within region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Grouping the types of categorical variables\n",
    "The features dealing with the management of the well include:\n",
    "- funder\n",
    "- installer\n",
    "- scheme_management\n",
    "- management/management_group\n",
    "- payment_type\n",
    "- permit (T/F)\n",
    "\n",
    "The features dealing with the creation of the well include:\n",
    "- extraction/extraction_type_class\n",
    "- waterpoint_type\n",
    "\n",
    "The features dealing with time include: \n",
    "- construction_year\n",
    "- date_recorded_offset_days (int; how long ago it was constructed, from the date recorded)\n",
    "- date_recorded_month\n",
    "\n",
    "The features dealing with natural properties include:\n",
    "- water_quality\n",
    "- source/source_class\n",
    "- amount_tsh\n",
    "- quantity\n",
    "- basin\n",
    "\n",
    "The features dealing with the community using the well and location include:\n",
    "- subvillage\n",
    "- region\n",
    "- population\n",
    "- public_meeting (T/F)\n",
    "- gps_height\n",
    "- longitude\n",
    "- latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'amount_tsh', u'funder', u'gps_height', u'installer', u'basin',\n",
       "       u'subvillage', u'region', u'population', u'public_meeting',\n",
       "       u'scheme_management', u'permit', u'extraction_type_group',\n",
       "       u'extraction_type_class', u'management', u'management_group',\n",
       "       u'payment_type', u'water_quality', u'quantity', u'source_type',\n",
       "       u'source_class', u'waterpoint_type_group', u'date_recorded_offset_days',\n",
       "       u'date_recorded_month', u'status_group'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Splitting the training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train, test = train_test_split(xrange(df_new.shape[0]), train_size=0.7)\n",
    "mask=np.ones(df_new.shape[0])\n",
    "mask[train]=1\n",
    "mask[test]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Standardize\n",
    "Now standardize our quantitative variables so they can be compared. These variables are amount_tsh, gps_height, population, and date_recorded_offset_days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>population</th>\n",
       "      <th>date_recorded_offset_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.831495</td>\n",
       "      <td>1.042939</td>\n",
       "      <td>-0.148715</td>\n",
       "      <td>1.135927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>1.055942</td>\n",
       "      <td>0.205933</td>\n",
       "      <td>-1.021636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.093236</td>\n",
       "      <td>0.025834</td>\n",
       "      <td>0.143714</td>\n",
       "      <td>-0.994778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>-0.585296</td>\n",
       "      <td>-0.254488</td>\n",
       "      <td>-0.911221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>-0.965266</td>\n",
       "      <td>-0.374778</td>\n",
       "      <td>0.774841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount_tsh  gps_height  population  date_recorded_offset_days\n",
       "0    1.831495    1.042939   -0.148715                   1.135927\n",
       "1   -0.101290    1.055942    0.205933                  -1.021636\n",
       "2   -0.093236    0.025834    0.143714                  -0.994778\n",
       "3   -0.101290   -0.585296   -0.254488                  -0.911221\n",
       "4   -0.101290   -0.965266   -0.374778                   0.774841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing as prepr\n",
    "STANDARDIZABLE = [u'amount_tsh', u'gps_height', u'population', u'date_recorded_offset_days']\n",
    "scaler = prepr.StandardScaler().fit(df_new[mask][STANDARDIZABLE])\n",
    "df_new[STANDARDIZABLE] = scaler.transform(df_new[STANDARDIZABLE])\n",
    "df_new[STANDARDIZABLE].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualizing the feature variables\n",
    "Let's plot these variables now and see how their distributions differ across the functional, non functional, and needs repair wells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "# for i, feature in enumerate(STANDARDIZABLE):\n",
    "#     ax = axes[i/2, i%2]\n",
    "#     sns.kdeplot(df_new[df_new['status_group']=='functional'][feature], ax=ax, shade=True,color='green', label=\"Functional wells\")\n",
    "#     sns.kdeplot(df_new[df_new['status_group']=='non functional'][feature], ax=ax, shade=True,color='red', label=\"Non Functional Wells\")\n",
    "#     sns.kdeplot(df_new[df_new['status_group']=='functional needs repair'][feature], ax=ax, shade=True,color='blue', label=\"Wells that Need Repair\")\n",
    "#     ax.set_title(feature)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CATEGORICAL = [u'funder', u'installer', u'basin', u'region',\n",
    "               u'public_meeting', u'scheme_management', u'permit',\n",
    "               u'extraction_type_group', u'extraction_type_class',\n",
    "               u'management', u'management_group', u'payment_type', u'water_quality',\n",
    "               u'quantity', u'source_type', u'source_class', u'waterpoint_type_group', u'date_recorded_month']\n",
    "for variable in CATEGORICAL:\n",
    "    df_new[variable] = df_new[variable].astype('category')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig1, axes1 = plt.subplots(5,4)\n",
    "\n",
    "# plt.rc('xtick', labelsize=10) \n",
    "# plt.rc('ytick', labelsize=10) \n",
    "# for j, variable in enumerate(CATEGORICAL):\n",
    "#     ax1 = axes1[j/4,j%4]\n",
    "#     functional = df_new[df_new['status_group']=='functional'][variable].tolist()\n",
    "#     non_functional = df_new[df_new['status_group']=='non functional'][variable].tolist()\n",
    "#     needs_repair = df_new[df_new['status_group']=='functional needs repair'][variable].tolist()\n",
    "#     categories = pd.Series(df_new[variable].values.ravel()).unique()\n",
    "#     value_freq_funct = []\n",
    "#     value_freq_nonf = []\n",
    "#     value_freq_repair = []\n",
    "#     categories_num = range(1,len(categories)+1)\n",
    "#     for value in categories:\n",
    "#         value_freq_funct.append(functional.count(value))\n",
    "#         value_freq_nonf.append(non_functional.count(value))\n",
    "#         value_freq_repair.append(needs_repair.count(value))\n",
    "#     ax1.bar(categories_num,value_freq_funct,align='center',color='g')\n",
    "#     ax1.bar(categories_num,value_freq_nonf,align='center',color='r')\n",
    "#     ax1.bar(categories_num,value_freq_repair,align='center',color='b')\n",
    "#     ax1.set_xticklabels(categories)\n",
    "#     ax1.legend()\n",
    "#     ax1.set_title(variable)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at some of them. In particular, ___ seem to show very interesting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##To run\n",
    "#for subvillages\n",
    "# functional = df_new[df_new['status_group']=='functional']['subvillage'].tolist()\n",
    "# non_functional = df_new[df_new['status_group']=='non functional']['subvillage'].tolist()\n",
    "# needs_repair = df_new[df_new['status_group']=='functional needs repair']['subvillage'].tolist()\n",
    "\n",
    "# plt.rc('xtick', labelsize=12) \n",
    "# plt.rc('ytick', labelsize=12)\n",
    "\n",
    "# categories = pd.Series(df_new.subvillage.values.ravel()).unique()\n",
    "# value_freq_funct = []\n",
    "# value_freq_nonf = []\n",
    "# value_freq_repair = []\n",
    "# categories_num = range(1,len(categories)+1)\n",
    "# for value in categories:\n",
    "#     value_freq_funct.append(functional.count(value))\n",
    "#     value_freq_nonf.append(non_functional.count(value))\n",
    "#     value_freq_repair.append(needs_repair.count(value))\n",
    "# fig, [ax1, ax2, ax3] = plt.subplots(3, 1)\n",
    "# ax1.bar(categories_num,value_freq_funct,align='center',color='g')\n",
    "# ax2.bar(categories_num,value_freq_nonf,align='center',color='r')\n",
    "# ax3.bar(categories_num,value_freq_repair,align='center',color='b')\n",
    "#plt.xticks(categories_num, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x10d713410>,\n",
       "  <matplotlib.axis.XTick at 0x10d713910>,\n",
       "  <matplotlib.axis.XTick at 0x10cbc1850>,\n",
       "  <matplotlib.axis.XTick at 0x10cf870d0>,\n",
       "  <matplotlib.axis.XTick at 0x10cf87850>,\n",
       "  <matplotlib.axis.XTick at 0x10cf87fd0>,\n",
       "  <matplotlib.axis.XTick at 0x10cf90790>,\n",
       "  <matplotlib.axis.XTick at 0x10cf90f10>,\n",
       "  <matplotlib.axis.XTick at 0x10cf9a6d0>,\n",
       "  <matplotlib.axis.XTick at 0x10cf9ae50>,\n",
       "  <matplotlib.axis.XTick at 0x10cfa3610>],\n",
       " <a list of 11 Text xticklabel objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAwQAAAITCAYAAAC5X9t1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzs3XlAVWXi//EP4laUYaWAubRMIpphuaQJlIIYm4jmQpbL\n",
       "qGlSVtJoLu1uY45OaJmNmZOlYpbLz6EsRVOpFG03XCYVNy5o5g4o8Pz+8MsdSRRFCPB5v/6Sc+89\n",
       "PM/lLud9zzlXF2OMEQAAAAArVSrrAQAAAAAoOwQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQ\n",
       "AAAAABa7pCBYunSpIiMjFRUVpejoaG3ZskV5eXkaO3asQkJC1LFjRy1YsMB5/dTUVPXq1UthYWHq\n",
       "3r27du7c6bxs0aJFCg0NVceOHfXKK68oNzdXkpSVlaXY2FiFhoYqJCREK1euLOGpAgAAAPijykVd\n",
       "YdeuXZo8ebKWLFmim266SV9++aWefPJJDRw4UHv37lVCQoKOHz+uHj16qEmTJmratKmee+459evX\n",
       "T6GhoVq7dq2GDh2q5cuXa/v27Zo+fbqWLFkid3d3xcbGas6cOerfv7/i4uLk5uamhIQEpaWlqXv3\n",
       "7mratKk8PDz+jPsBAAAAsFKRewiqVq2qsWPH6qabbpIkNW3aVAcPHtSKFSvUpUsXubi4qEaNGgoL\n",
       "C9OyZcuUnp6uXbt2KTQ0VJIUEBCgzMxMpaSkKDExUYGBgXJ3d5ck9ejRQ8uWLZMkrVq1St27d5ck\n",
       "eXl5yc/PT59++mmpTBoAAADAWUXuIbjlllt0yy23OH+eMGGCAgMDtWPHDnl5eTmXe3h4aPv27XI4\n",
       "HKpdu3aBdXh4eMjhcCgtLU1169Z1Lvf09JTD4ZAkpaWlnbe+9PT04s8MAAAAQJEu+aTizMxMDR06\n",
       "VHv37tW4ceOcx/4XWFmlSsrLyyv8F1WqJGPMectdXV0lqdDbVarEOc8AAABAaSpyD4EkHThwQE88\n",
       "8YT+8pe/6P3331fVqlVVp04dZWRkOK+Tnp4uT09P1alTRwcPHixw+/zLvLy8ClyWv1yS83b5hyal\n",
       "p6ercePGlz2hzZs3X/ZtAAAAABs0b978vGVFBsHRo0f16KOPqmvXroqJiXEuDwwM1Mcff6x27drp\n",
       "5MmTSkhI0KuvvioPDw/Vr19fCQkJCg0N1bp16+Tq6ipvb29JUkxMjAYPHqyaNWsqPj5eQUFBzvXF\n",
       "x8frpZdeksPh0Pr16zVkyJASm+ifbd26dQp4L0BqUNYjKaZUaW2/tfL39y/rkUiSUlJSJEk+Pj5l\n",
       "PJLSZ9NcJbvma9NcJbvma9NcJbvma9NcJbvma9Nc813og/Mig2D+/PlKT0/XypUr9cUXX0iSXFxc\n",
       "9O6772rPnj2KjIzUmTNnFB0drRYtWkiSpk6dqtGjR2vGjBmqVq2a4uLiJEne3t6KiYlRnz59lJOT\n",
       "I19fXw0YMECS9NRTT+nll19WeHi48vLyNGLECNWrV69EJg8AAACgcEUGweDBgzV48OBCLxs1alSh\n",
       "y+vXr6+5c+cWellUVJSioqLOW37ttddq0qRJRQ0HAAAAQAnirF0AAADAYgQBAAAAYDGCAAAAALAY\n",
       "QQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEA\n",
       "AAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAA\n",
       "AFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABY\n",
       "jCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwg\n",
       "AAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAA\n",
       "AAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAA\n",
       "LEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxG\n",
       "EAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAA\n",
       "AAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAA\n",
       "ABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAW\n",
       "IwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMI\n",
       "AAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAA\n",
       "AACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAA\n",
       "ixEEAAAAgMUIAgAAAMBiBAEAAABgscqXesWRI0eqYcOG6tevnySpdevW8vLycl7ev39/hYeHKzU1\n",
       "VaNGjdKRI0fk5uamiRMn6vbbb5ckLVq0SLNnz1Zubq7uv/9+jRkzRq6ursrKytLo0aOVkpIiY4xi\n",
       "Y2MVFBRUwlMFAAAA8EdFBsGvv/6qV199VT/++KMaNmwoSdq1a5fc3d21ePHi867/3HPPqV+/fgoN\n",
       "DdXatWs1dOhQLV++XNu3b9f06dO1ZMkSubu7KzY2VnPmzFH//v0VFxcnNzc3JSQkKC0tTd27d1fT\n",
       "pk3l4eFR8jMGAAAA4FTkIUPz5s1T165d9dBDDzmXfffdd6pUqZJ69+6tTp066c0335QxRunp6dq1\n",
       "a5dCQ0MlSQEBAcrMzFRKSooSExMVGBgod3d3SVKPHj20bNkySdKqVavUvXt3SZKXl5f8/Pz06aef\n",
       "lvhkAQAAABRU5B6CF154QZL09ddfO5fl5uaqbdu2GjFihLKysjRw4EBdf/318vX1Ve3atQvc3sPD\n",
       "Qw6HQ2lpaapbt65zuaenpxwOhyQpLS2twOFHHh4eSk9PL/akUlJSin3bkrJ79+6yHsIV2717t26+\n",
       "+eayHoYkKTMzU1L5+NuWNpvmKtk1X5vmKtk1X5vmKtk1X5vmKtk1X5vmWpRLPofgXN26dXP++7rr\n",
       "rlO/fv00d+5cNW3atNDrV6pUScaY85a7urpKkvLy8gq9DQAAAIDSVawgWLp0qRo1aiRvb29JkjFG\n",
       "VapUUZ06dXTw4MEC101PT5enp6e8vLwKXJa/XJLzdjfddJPzssaNGxdrQpLk4+NT7NuWlEOHDpX1\n",
       "EK7YrbfeWi7uS+l/9V5exlOabJqrZNd8bZqrZNd8bZqrZNd8bZqrZNd8bZprvs2bNxe6vFgfw+/Y\n",
       "sUPTpk1TXl6esrKy9MEHHyg0NFQeHh6qX7++EhISJEnr1q2Tq6urvL291b59eyUmJurw4cMyxig+\n",
       "Pt75TUKBgYGKj4+XJDkcDq1fv14PPvhgcYYGAAAA4DIUKwiefPJJ3XDDDYqIiFBkZKSaN2+uhx9+\n",
       "WJI0depUzZ8/XxEREXrjjTcUFxcnSfL29lZMTIz69Omj0NBQVa5cWQMGDJAkPfXUUzp58qTCw8P1\n",
       "17/+VSNGjFC9evVKaIoAAAAALuSSDxmaMGGC89/Vq1fXuHHjCr1e/fr1NXfu3EIvi4qKUlRU1HnL\n",
       "r732Wk2aNOlShwIAAACghHDmLgAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAA\n",
       "ixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsR\n",
       "BAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQA\n",
       "AACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAA\n",
       "gMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDF\n",
       "CAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgC\n",
       "AAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAA\n",
       "AMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADA\n",
       "YgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABarXNYDuFplZ2dLjrIexRVw/N8cAAAA\n",
       "cFUjCErJ1q1bNe1TybesB1JMP0jaGrpVQUFBZT0UAAAAlCKCoBT5SvIv60FcgR/KegAAAAAodZxD\n",
       "AAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAA\n",
       "AABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAA\n",
       "WIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiM\n",
       "IAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAA\n",
       "AAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAA\n",
       "ACx2yUEwcuRIvffee5KkvLw8jRs3TiEhIerYsaMWLFjgvF5qaqp69eqlsLAwde/eXTt37nRetmjR\n",
       "IoWGhqpjx4565ZVXlJubK0nKyspSbGysQkNDFRISopUrV5bU/AAAAABcRJFB8Ouvv6pPnz767LPP\n",
       "nMvmz5+vPXv2KCEhQR999JH+/e9/66effpIkPffcc+rVq5f+85//6Mknn9TQoUMlSdu3b9f06dM1\n",
       "b948rVixQseOHdOcOXMkSXFxcXJzc1NCQoJmz56tV155Renp6aUwXQAAAADnKjII5s2bp65du+qh\n",
       "hx5yLlu1apW6dOkiFxcX1ahRQ2FhYVq2bJnS09O1a9cuhYaGSpICAgKUmZmplJQUJSYmKjAwUO7u\n",
       "7pKkHj16aNmyZc71de/eXZLk5eUlPz8/ffrppyU+WQAAAAAFVS7qCi+88IIk6euvv3YuS0tLk5eX\n",
       "l/NnDw8Pbd++XQ6HQ7Vr1y5wew8PDzkcDqWlpalu3brO5Z6ennI4HBdc35XsIUhJSSn2bUuKw+GQ\n",
       "b1kP4go5HI5ycV9KUmZmpqTy8bctbTbNVbJrvjbNVbJrvjbNVbJrvjbNVbJrvjbNtSjFOqk4Ly/v\n",
       "/BVVqlTo8vzLjDHnLXd1db3o+gAAAACUriL3EBSmTp06ysjIcP6cnp4uT09P1alTRwcPHixw3fzL\n",
       "vLy8ClyWvzx/fQcPHtRNN93kvKxx48bFGZokycfHp9i3LSn5c6vIPD09y8V9Kf2v3svLeEqTTXOV\n",
       "7JqvTXOV7JqvTXOV7JqvTXOV7JqvTXPNt3nz5kKXF+tj+MDAQH388cfKzc3VsWPHlJCQoKCgIHl4\n",
       "eKh+/fpKSEiQJK1bt06urq7y9vZW+/btlZiYqMOHD8sYo/j4eAUFBTnXFx8fL+nsYSrr16/Xgw8+\n",
       "WJyhAQAAALgMxdpDEB0drb179yoyMlJnzpxRdHS0WrRoIUmaOnWqRo8erRkzZqhatWqKi4uTJHl7\n",
       "eysmJkZ9+vRRTk6OfH19NWDAAEnSU089pZdfflnh4eHKy8vTiBEjVK9evRKaIgAAAIALueQgmDBh\n",
       "gvPfrq6uGjlyZKHXq1+/vubOnVvoZVFRUYqKijpv+bXXXqtJkyZd6lAAAAAAlBDO3AUAAAAsRhAA\n",
       "AAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAA\n",
       "ABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAW\n",
       "IwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMI\n",
       "AAAAAIsRBAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAA\n",
       "AACLEQQAAACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAA\n",
       "ixEEAAAAgMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsR\n",
       "BAAAAIDFCAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQA\n",
       "AACAxQgCAAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAA\n",
       "gMUIAgAAAMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDF\n",
       "CAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgC\n",
       "AAAAwGIEAQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAA\n",
       "AMBiBAEAAABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADA\n",
       "YgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIE\n",
       "AQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxW+UpuPHHiRK1YsULu7u6SpNtuu02TJ0/W+PHjlZSU\n",
       "pLy8PPXr1089e/aUJKWmpmrUqFE6cuSI3NzcNHHiRN1+++2SpEWLFmn27NnKzc3V/fffrzFjxsjV\n",
       "1fUKpwcAAADgYq4oCL7//ntNnTpVzZo1cy6bN2+e9u7dq4SEBB0/flw9evRQkyZN1LRpUz333HPq\n",
       "16+fQkNDtXbtWg0dOlTLly/X9u3bNX36dC1ZskTu7u6KjY3VnDlz1L9//yueIAAAAIALK/YhQ6dP\n",
       "n9Yvv/yi2bNnKzIyUkOHDlVaWppWrlypLl26yMXFRTVq1FBYWJiWLVum9PR07dq1S6GhoZKkgIAA\n",
       "ZWZmKiUlRYmJiQoMDHTuaejRo4eWLl1aMjMEAAAAcEHFDoKMjAy1adNGsbGxWrp0qZo1a6YhQ4Yo\n",
       "LS1NXl5ezut5eHgoPT1dDodDtWvXLrAODw8PORwOpaWlydPT07nc09NT6enpxR0aAAAAgEtU7EOG\n",
       "6tatq5kzZzp//utf/6o333xT2dnZ5123UqVKysvLK3Q9lSpVkjHmvOVXcv5ASkpKsW9bUhwOh3zL\n",
       "ehBXyOFwlIv7UpIyMzMllY+/bWmzaa6SXfO1aa6SXfO1aa6SXfO1aa6SXfO1aa5FKfYegm3btp13\n",
       "WI8xRq1atVJGRoZzWXp6ujw9PVWnTh0dPHiwwPXzL/Py8ipwWf5yAAAAAKWr2HsIKlWqpPHjx6tF\n",
       "ixa65ZZb9OGHH6pRo0YKDAzUokWL1K5dO508eVIJCQl69dVX5eHhofr16yshIUGhoaFat26dXF1d\n",
       "5e3tLUmKiYnR4MGDVbNmTcXHxyswMLDYk/Lx8Sn2bUvK1RA0np6e5eK+lP5X7+VlPKXJprlKds3X\n",
       "prlKds3XprlKds3XprlKds3Xprnm27x5c6HLix0Ed955p8aMGaPBgwcrLy9Pnp6emjJlimrVqqXU\n",
       "1FRFRkbqzJkzio6OVosWLSRJU6dO1ejRozVjxgxVq1ZNcXFxkiRvb2/FxMSoT58+ysnJka+vrwYO\n",
       "HFjcoQEAAAC4RFf0taMRERGKiIg4b/moUaMKvX79+vU1d+7cQi+LiopSVFTUlQwHAAAAwGXifyoG\n",
       "AAAALEYQAAAAABYjCAAAAACLXdE5BAAAVARZWVlKTk4utfXv3r1bknTo0KFS+x0tW7ZU9erVS239\n",
       "AOxFEAAArnrJycn6ISCg1P7DyFtLab35fpCktWvl7+9fyr8JgI0IAgCAFXwlsTkNAOfjHAIAAADA\n",
       "YgQBAAAAYDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIE\n",
       "AQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEA\n",
       "AABgMYIAAAAAsBhBAAAAAFiMIAAAXPWys7PLeghX7GqYA4DyiSAAAFz1tm7dWtZDuGJXwxwAlE8E\n",
       "AQAAAGAxggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEA\n",
       "AABgMYIAAAAAsBhBAAAAAFiMIAAAAAAsRhAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYgQBAAAA\n",
       "YDGCAAAAALAYQQAAAABYjCAAAAAALEYQAAAAABYjCAAAAACLEQQAAACAxQgCAAAAwGIEAQAAAGAx\n",
       "ggAAAACwGEEAAAAAWIwgAAAAACxGEAAAAAAWIwgAAAAAixEEAAAAgMUIAgAAAMBiBAEAAABgMYIA\n",
       "AAAAsBhBAAAAAFisclkP4Gp15swZ/VDWg7gCP+jsHAAAAHB1IwhKSWpqqt7QNEm+ZT2UYvpBT6f+\n",
       "t6wHAQAAgFJGEJQqX0n+ZT2IK0AQAAAAXO04hwAAAACwGEEAAAAAWIwgAAAAACzGOQQAAFxlsrKy\n",
       "lJycXGrr3717tyTp0KFDpbL+li1bqnr16qWybgDnIwgAALjKJCcnK2BCgORZ1iMpBoe0duRa+ftX\n",
       "5C/lACoWggAAgKuRp6QGZT0IABUB5xAAAAAAFiMIAAAAAIsRBAAAAIDFCAIAAADAYpxUDOCi+PpC\n",
       "AACubgQBgIvi6wsBALi6EQQAisbXFwIAcNXiHAIAAADAYgQBAAAAYDGCAAAAALAYQQAAAABYjCAA\n",
       "AAAALEYQAAAAABYjCAAAAACLEQQAAACAxfiPyQBcVHZ2tuQo61EUk+P/xg8AAC6IIABwUVu3btW0\n",
       "TyXfsh5IMfwgaWvoVgUFBZX1UIA/FSEP4HIQBACK5CvJv6wHUUw/lPUAgDJAyAO4HAQBAABXIUIe\n",
       "wKXipGIAAADAYuwhAABLZWVlKTk5udTWv3v3bknSoUOHSmX9LVu2VPXq1Utl3QBgE4IAACyVnJys\n",
       "HwICSu0481tLab3S/x1Ssnat/P0r6kExAFB+EAQAYLGKfJw5AKBkcA4BAFiqon+1Y0UfPwCUFwQB\n",
       "AFhq69atZT2EK1LRxw8A5QVBAAAAAFiMcwiAy1TRv5lF4ttZAADA/xAEwGVKTk5WwIQAybOsR1JM\n",
       "DmntSL6dpTDEHgDARgQBUByekhqU9SBQ0oi9q9eZM2cq9P9++4POzgHnI+SvXvxt/zwEAQCci9i7\n",
       "KqWmpuoNTZNK7X9dKG0/6OnU/5b1IMolQv7qVZH/rxSpYv1/KQQBAMASFf1/XSAILoiQv2pV9Gdt\n",
       "RUEQoERU9N16FWWXHgAAQEkjCFAiKvQuW3bXAgBQ7mRnZ6taWQ/iClWU/0CRIEDJYZctAOBPlp2d\n",
       "LTnKehRXwHF5G40VfY+8dOl75bdu3Vphz/rJt3XrVgUFBZX1MIpEEACXybY3HwAoz7Zu3appn1bk\n",
       "08WlraGXvtFYoffIS+yVL6cIAuAy2fbmA6Diqchfs1qcr1it6CeeXvbfij3yKGEEAVAM1r35WIK9\n",
       "P7haVOyvWeUrVoE/G0GAElGhN6TYiML/Ye8Pri4V+aMLggD4MxEEKBEVeUOKjSicqyJvQkns/QGu\n",
       "dhX6AziJD+HKKYIAJaYib0ixEQUAqAgq8gdwEh/ClVcEAQBYyrYTT4GrRUX+AE7iQ7jyiCAAAEtx\n",
       "4ikAQCpnQbBmzRpNmTJFZ86ckbe3t8aNGyc3N7eyHhZgNZs+Ra7Ic5WK+6l5Rf6skSAAgJJQboLg\n",
       "8OHDGjVqlOLj41WvXj1NnjxZkydP1ksvvVTWQ8MlqMgbUmw0XpxNnyJX7LlKfGoO4Gpi2/ttWSo3\n",
       "QZCUlKS7775b9erVkyRFR0crMjKSIKggKvaGFBuNRbPpU+SKPFeJT81hG9s2Gm2ar53vt2Wj3ARB\n",
       "WlqaPD3/9/9we3p66uTJkzp58iSHDVUYFXlDio1GAKiIbNtotG2+vN/+OcpNEBhjCl3u6up62etK\n",
       "SUm50uFcscOHD6tin0f/gw4fPnzJ92XFnq9Nc5Xsmq9Nc5Xsmq9Nc5Xsmm9x5lqxMd8LX6/iPo6l\n",
       "y30slyUXc6Et8T/ZsmXL9Nlnn+mtt96SJO3fv19dunTRhg0bLms9mzdvLo3hAQAAABVe8+bNz1tW\n",
       "bvYQ+Pn5adKkSdqzZ4/q16+v+Ph4BQYGXvZ6CpskAAAAgMKVmz0EkrR27Vr94x//UE5OjurVq6dJ\n",
       "kyapRo0aZT0sAAAA4KpVroIAAAAAwJ+rUlkPAAAAAEDZIQgAAAAAixEEAAAAgMUIAgAAAMBiBAEA\n",
       "AABgMYIAAAAAsBhBAAAAAFiMIChH5s+fr8jISIWHhysiIkIjRoxQWlqaJOmjjz7S/PnzJUnTp0/X\n",
       "2LFjy3KoxdKoUSMdOXKkwLIVK1boscceK/K2gwYN0q+//lqs33vixAn17NlTERER+uKLL4q1jsvR\n",
       "qFEjderUSZ07d1ZUVJQeeughdevWTT///HOp/+4/2/79+9W4cWNFRUUpKipKnTp1UteuXbVkyZJi\n",
       "rzMxMVGhlv/gAAAaGUlEQVTjxo0r9LKIiAglJycXe90DBw7U+++/7/x59+7datSokaZOnepcdvjw\n",
       "Yd111106ceLEJa9348aNioiIKPSywYMHn3d/7Nu3T3fddZcyMjLOu36nTp20cuVKxcXFaenSpRf9\n",
       "vVFRUZc1zqIU9hxdvHixBg8eXORtP/roI3Xv3l1hYWEKDg5W//799eOPP5bY2Mqjffv2aejQoRe8\n",
       "fO/evRo2bJiCg4PVuXNnde/eXR9//LHz8ry8PD3xxBN66KGH9OGHHzqXHz9+3Pn6ERwcLF9fX+dz\n",
       "7PXXXy/VOV2qAQMGaPfu3WU9jCuyf/9+3XPPPQWWJSQkqE2bNvr6668LfU5f7Lnev3//854/ZWns\n",
       "2LHq3LmzOnfurLvuukshISHOx9Xp06dL7Pf88ssvCgoKUteuXZWenl6sdZSX12bp7DZWmzZtFBUV\n",
       "pcjISIWFhelvf/ubTp06dRkzKuhSX0fLUuWyHgDO+vvf/67t27frnXfekYeHhyRpyZIl6tmzpxYu\n",
       "XKhvv/1WDRs2LONRXhkXF5fLWn6umTNnFvv3pqSk6Pfff9eKFSuKvY7L4eLiorlz5+qGG25wLps9\n",
       "e7bGjh2rBQsW/Clj+DNVr15dixcvdv584MAB9e3bV25uburQocNlr699+/Zq3759SQ7RKSAgQBs2\n",
       "bFDv3r0lSatXr1b79u2VmJioZ599VpL0zTffqHnz5rruuutKZQySVLduXfn5+Wnx4sUaNGiQc/l3\n",
       "332nEydOKDAwUEFBQUWu59z7vSRcynOxMFOmTNHmzZsVFxcnT09PSWfvx0GDBmnx4sXOZVeb/fv3\n",
       "a9euXRe87LHHHtOzzz6rKVOmSJIyMjIUGxur1NRUDRs2TA6HQ1999ZW+//77Avf99ddf79xQ2bhx\n",
       "o1577bUS/1tfqVmzZpX1EErEuff7ggUL9Pbbb2vOnDk6evToZa8rKSmpJId2xcaMGeP8d2BgoP7x\n",
       "j3+ocePGJf57Vq5cKT8/P7388svFXkd5eW3OFxYW5rz/jDF64oknNHfu3AKv11cbgqAcSE9P14IF\n",
       "C7Ru3boCD/TOnTtry5Yt6t+/vw4ePKivvvpK1apVkyT9+uuv6t27tw4ePKibb75ZU6dO1c0336z0\n",
       "9HS99tprSktLU05OjsLCwvT4449r//796tWrl+644w7t379fH3zwgW6++eY/dZ5F/afY06dP1/79\n",
       "+5WRkaEDBw7oxhtv1D//+U/VqlVL7du317Rp09SkSRMtWrRIc+bMkaurq2rWrKmJEyfK09NTq1ev\n",
       "1owZM5STk6Pq1atr+PDhuuGGGzR69GhlZGQoKipK8fHxqlq1aqnP89y55ubm6sCBA3J3d9eRI0fU\n",
       "rl07ff3116pevbpefPFF7dy5Ux988IEkqWPHjnrrrbe0YcMG51irVaumV155RXfccYfmzZtX6PL2\n",
       "7durQ4cO2rRpk06cOKG+ffsqOjq6VOd5IXXq1NHQoUM1a9Ys3XnnnXr11Vd16tQpZWRkyMfHR1On\n",
       "TlXVqlV199136/HHH1dSUpIOHjyo3r17q3fv3lq8eLFWrFiht99+W//97381evRoZWVl6bbbblNm\n",
       "Zqbz97z99ttatWqVTp8+rczMTA0fPrzIjeiAgABNnz7d+XNiYqJiY2M1bNgw7du3T3Xr1tXXX3+t\n",
       "Bx98UJK0Y8cOvfbaazpy5IgqVaqkvn37qnPnztq4caPGjRuna665RllZWXruueec68zIyNDzzz+v\n",
       "gwcPysvLS7/99luhY4mOjta4ceMKvMEsXLhQPXv2lIuLi0aOHKmGDRuqX79+iouL06pVq1SlShW5\n",
       "u7tr4sSJuvnmm9WoUSN98803cnd315tvvqmEhARVrlxZt956q1588UXddNNNeuyxx3TPPffo22+/\n",
       "1YEDB9SiRQtNmjSp0DEV9Rw9c+aMJk+erOTkZOXl5cnHx0dDhgzR+++/r5iYGA0aNKjAY3PkyJFa\n",
       "vXq1li5dqhMnTig1NVXXX3+9ateuraNHj2r8+PFatGiRKlWqpBUrVmjTpk365JNPNGvWLPXv318u\n",
       "Li56/fXXlZubq9zcXPn6+mr27Nl655139N133+nQoUPy9vZW/fr1L/raERERoTVr1ujo0aN68skn\n",
       "9e2332rLli2qUqWKZsyYoVq1al30tbNv37564IEH9MMPP+jYsWN65pln1LFjR73wwgvKyMjQgAED\n",
       "zttAnjlzpiIiIhQZGelcVrt2bf3zn/9UUFCQevXqpYEDByonJ0ddunRRXFyc6tWrd9H7P9+pU6f0\n",
       "0ksvae/evfr99991/fXXa+rUqapXr54eeeQRtWzZUps3b9aBAwd03333acKECVqxYoVmzJghFxcX\n",
       "GWOUmpqqkJAQjR8/Xm+++abWrFnjfC517dpVa9as0YkTJ5SRkaHTp0+rRo0aql27tqpVq6acnBz9\n",
       "+OOPatGihd555x1NnjxZH3zwgWrWrKlTp07phhtuUP/+/dWnT59Lmk958M4772jJkiWaP3++vLy8\n",
       "tHHjRudlmzZt0vDhw51hV9jy+Ph4SVLv3r31r3/9y/nBXnnxx/elhQsXatGiRcrJydGRI0c0ePBg\n",
       "de/eXRkZGRoxYoQziNq3b68nn3yy0MfclClT9O2332rhwoXKy8tTVlaWJk6cWKzxlafX5j/KzMxU\n",
       "ZmamatWqJens3ovLfV8712effaYpU6bonXfe0a233lqs+6tUGJS5FStWmIcffrjQyxITE02nTp3M\n",
       "888/b2bPnm2MMWbatGkmKCjI/P7778YYY4YMGWLeeustY4wxvXv3NqtXrzbGGJOdnW169+5tPv30\n",
       "U7Nv3z7j7e1tNm/eXPoTugBvb2/nmPN99tln5rHHHjPGnJ1Xhw4dzMmTJ40xxgwePNhMmzbNGGNM\n",
       "u3btzM8//2xSUlJM69atjcPhMMYY8+9//9u89NJLZvfu3SY8PNwcOXLEGGPMjh07TNu2bU1mZqbZ\n",
       "sGGDCQ8P/7Omaby9vU1ERITp1KmT8fPzM4GBgWbs2LHmt99+M8YY06dPH7NmzRpjjDEdO3Y0bdu2\n",
       "NadOnTI7duwwYWFhJjc319x1113m4MGDxhhjli5dahYuXHjB5fn3z4svvmiMMcbhcJjWrVub7du3\n",
       "l/pc9+3bZ+65557zlu/YscM0a9bMTJo0ySxbtswYY8yZM2dMRESE+fzzz40xZ++nDz/80BhjzM8/\n",
       "/2yaNm1qsrOzzSeffGIGDRpkjDGmc+fO5uOPPzbGGLN582bj4+NjNm7caPbv32/69OljsrOzjTHG\n",
       "/Oc//7nkv3FwcLBJSUkxR48eNX5+fsYYY1588UUzZ84cY4wxgYGBZufOnSYnJ8cEBQWZL774whhj\n",
       "THp6ugkICDDff/+92bBhg2ncuLFJS0szxpgCj7EhQ4aYN954wxhjTGpqqmnWrJlZvHjxeePIy8sz\n",
       "HTp0MBs3bjTGGHP8+HHTqlUrc/jwYWOMcT7n09LSTPPmzc3p06eNMca89957ZuXKlcYYYxo1amR+\n",
       "//13s2jRItOzZ0+TlZVljDn7XOrfv78xxphHH33UPPPMM8YYY06cOGH8/f3Nhg0bCr1v8h+7nTt3\n",
       "Np07dzaRkZHmwQcfdP49pk+fbiZNmuS8/pQpU8yAAQNMly5dLvjY7N27t9m5c6cJCgoyixYtMo0b\n",
       "NzY7duww9957r4mNjTXffPONadGihfHz8zNJSUmmW7duplWrVub777839913n1mwYIExxpiUlBTj\n",
       "4+Njli9fbqZNm2ZCQkJMXl6ec74Xe+2YOHGiMebs48THx8ds27bNGGNMTEyMmTlzpnOcF3vtzH/O\n",
       "rlixwrRr1+68v/sfhYeHO2/zR1FRUeaLL7644PPnXIX9jv/85z9mwoQJzp/HjBnj/Dk6OtrExsYa\n",
       "Y84+ptq2bWs2bdpU4PZffPGF6dixo/n999/Nnj17TL9+/ZzPpaVLl5rg4GDTpEkTM2bMGBMcHGxm\n",
       "zpxpHn30URMYGGiGDh1qjDHG39/fBAcHmxkzZphhw4Y5n89vvfWW6d27t/P5XJ7t27fP+TrVqFEj\n",
       "M2/ePOdl+ff7N998Yzp06OB8Pb3QcmPOPn/y34PKm/z3UGPOPi569uxpjh49aowxZtOmTaZly5bG\n",
       "GGPi4uLMa6+9Zowx5uTJk+aZZ54xJ0+evOhjburUqWb8+PFXPMby8to8bdo007p1a9O5c2cTERFh\n",
       "mjdvbiIiIszx48eNMcb8/e9/L/b72v/7f//PhIeHO7dhyhP2EJQTOTk5hS4/ffp0obvx77//frm7\n",
       "u0s6e9zvb7/9pszMTCUnJ+vYsWP65z//Kels2aakpKhp06aqXLmymjVrVnqTKEJh88jLy5Orq6vz\n",
       "51atWunaa6+VJDVu3Pi84zG/+eYb+fv7Oz99yS/vefPm6dChQ+rbt6/zU5DKlSsrNTW1VOZSlPxD\n",
       "hlJSUjRw4EDdc889uvHGGyVJQUFBWrt2rerVqycPDw81bNhQGzdu1LZt2xQcHKxKlSopJCREPXr0\n",
       "0IMPPqi2bdsqIiJCLi4uhS7P16tXL0mSh4eH/P39lZSUpDvvvLNM5u/i4qJrrrlGf/vb37R+/XrN\n",
       "mjVLu3fv1sGDB3Xy5Enn9QIDAyVJTZo00ZkzZwrsAThy5Ii2bdvm/IT13nvv1V/+8hdJZ/dCTJw4\n",
       "UUuXLtWePXv0/fffF7jtxfj7+2vjxo268cYb1bZtW0lSu3btNG/ePAUFBcnFxUW33Xabfv31V50+\n",
       "fdq516F27doKDg7WunXr1KpVK3l6ehZ6KMzXX3+t559/XpJUv359tW7d+oL3UY8ePbRo0SK1bNlS\n",
       "S5cu1QMPPKCaNWsWuJ6Hh4d8fHwUFRUlf39/BQQEqE2bNgWus27dOnXp0sW5B7F37956++23na8r\n",
       "7dq1kyS5ubmpQYMGFz0c4o+Hu+XvsZGkNWvW6Pjx485DI3JycuTi4qJq1ao5H5tt27ZVUlKSatSo\n",
       "oX/961/q0KGD1qxZo8OHDztv5+bmpvbt2yspKUmTJ0/WqVOnFBUVpeXLl8vhcOjOO+/Uli1b5OLi\n",
       "olmzZmn69Ok6ffq0jDHas2ePJMnX17fAa8rFXjuCg4Odf49atWo5D7+sV6+ejhw5UuRrZ5UqVfTA\n",
       "Aw84132ph5Nc6HX9zJkzxT48S5JCQ0PVoEEDzZ07V6mpqUpOTlarVq2cl+cfcnfdddepXr16Bca7\n",
       "efNmjR07Vv/+97/l7u4ud3d3jRs3zvlc+u6775SVlaU6deropptuUuvWreXr66vly5crLCxMO3fu\n",
       "1KxZs3Ts2DFlZWXJ09NTwcHBWr58ubZu3aqff/5ZNWvWdD6fS3uP7JXKzMzUf//7X73zzjt65pln\n",
       "dM8996hRo0aSJIfDocGDBys6OrrAa+mFlktF72UrD6677jq9+eabSkxM1O7du5WSkuI8Pj4gIECD\n",
       "Bw/W3r171aZNGw0fPlzXXnttkY+5klBeXpulgocM5ebm6vXXX9fTTz+td999V3/729+UlJR02e9r\n",
       "P/30k9avX6+RI0eWuz1IEicVlwu+vr7avXt3obuvNmzYoHvvvfe85VWqVHH+O/+NJTc3V5IUHx+v\n",
       "JUuWaMmSJVqwYIHzRJaqVauqUqWy+5PfeOON523g//bbb86wkc4ej56vsDdMV1fXAsuzs7O1c+dO\n",
       "5eXlqU2bNlq8eHGBuZfVeRf5bwo+Pj4aOXKkRo8erQMHDkiSOnTooC+//FLr169X27Zt1bZtW61f\n",
       "v16JiYnODZdJkyZp5syZatCggf71r38pJiam0OVPPvmk83eeG1Z5eXll+rf+6aef1LBhQz377LNa\n",
       "uHChbrnlFvXr1++841fzN2Dznftm6uLi4jy8IV/lymc/w/jll1/Us2dPnTx5Un5+fho4cOAlvxH7\n",
       "+/srOTlZq1evdu5+bt26tX755Rd99dVXzg2/vLy8825rjNGZM2ckybnx+Ud/fNzmj7kwXbt21Zdf\n",
       "fqkTJ07oo48+0qOPPlro+ubOnauJEyeqZs2amjBhgsaPH1/gOn8ca/4hNvn3ybnPq/x5XMjFLsvN\n",
       "zdXo0aOdz7GPPvpIU6dO1c6dOzV69GjNnDlTf/nLX1S7dm15eXkpJCRE8fHx2rFjhypXrqzhw4fL\n",
       "1dVVxhjdcMMNysnJUWJiourWrasjR45o9erVatCggYKDg5WXl6dq1arJx8dHo0aN0rx589S8eXPn\n",
       "G6mbm1uBsV3stePcjdLC/h5FvXb+8fX2Uh5r9957rzZs2HDe8vT0dO3bt0933313keu4kLlz5+qF\n",
       "F16Qm5ubOnXqpJCQkAJjOvd5de54f/31V+c5DQ0aNJAk/fzzz4qOjtapU6fk7++vAQMGFFhH9erV\n",
       "netYuXKltm3bpltuuUVubm667bbbtGXLFj3xxBNycXFRUFCQevTo4fx9FWHj+JprrtGMGTPk7++v\n",
       "QYMGKSYmRseOHZN09rHy3nvvafHixfrpp5+ct7nQ8oriwIED6tKli9LT09WyZUs9/fTTzst8fX21\n",
       "atUqdevWTfv27VPXrl31448/FvmYKwnl6bX5XK6ururWrZs2b94sScV+X6tRo4beffddTZs2zbk9\n",
       "UJ4QBOWAh4eHevfurWHDhhU4Q//jjz/W559/roEDB8rV1dX5YL+Q6667Tr6+vnr33XclSceOHVN0\n",
       "dLRWrVolqexfnAMCAjR37lznOI4eParFixc7n+SX4r777tNXX32lQ4cOSTr7zUyTJ09WmzZtlJSU\n",
       "pJ07d0qSvvzyS0VGRio7O7vkJ3KZwsLCdO+99zq/OcfDw0M1a9ZUfHy8/Pz8dP/99+vzzz/XkSNH\n",
       "1KhRI/3+++968MEH5e7urt69e+uZZ57Rtm3bCl2+detW5+/J/0aaAwcO6KuvvlJAQMCfMr8/Pq52\n",
       "7dqlGTNmqF+/fkpKSlJMTIzzzeOHH35wbnwVtZ4bbrhBTZo00UcffSRJ2rJli7Zv3y5JSk5OVtOm\n",
       "TdW3b1+1bNlSK1euLPRNojCtW7dWSkqKNm3aJD8/P0lnN3qaNGmiDz/80PlGdNttt6lKlSpauXKl\n",
       "pLMbcitWrHB+cnUhAQEBzuOJDxw4UOhGYT53d3e1a9dO06ZNk6ura6EbiVu3blV4eLjuuOMOPf74\n",
       "4+rbt6/z755/n/n7++uTTz5xfhI1d+5ctWzZssCGbEnw9/fXhx9+qDNnzigvL0+jR4/W+++/r+7d\n",
       "u8vPz0/Z2dnOx+aWLVv0zTffKCcnRy+++KKuv/56vfvuuzpz5ozzvvTz89PkyZPVqVMnbdq0SUeP\n",
       "HtWePXvUsWNHtW7dWg6HQ5GRkQoJCdHGjRuVnJxcot+Oku9yXzvzf3Z1db3gXoAnnnhCn332WYFv\n",
       "MXE4HIqNjVWvXr2cxyMX53U5KSlJDz/8sLp06aIGDRpozZo1F3xe5cvIyNDjjz+ukSNHFviQaePG\n",
       "jWrWrJn69OmjFi1a6Isvvrjguvbu3atmzZopJCRE0tnjuHfs2OH8pNbb21srV64sEKMVQf6HJ48/\n",
       "/rjuvPNOPfvss8rLy9PNN9+sZs2aacSIEXruueec7ycXWl65cuULPh7Kk59++km1a9fWoEGD1LZt\n",
       "WyUmJjpfP/M/dAoKCtKYMWN0++23KzU1tViPuctVnl6b/+jzzz93vj4X932tQYMGuu+++/Too49q\n",
       "+PDhl/y7/ywcMlROPPvss/r44481ZMgQnT59WqdPn9bdd9+thQsXysvLSwEBAXrttdeKXM/kyZP1\n",
       "2muvKSIiQjk5OYqIiFB4eLj2799/RbuoS8KoUaM0ceJEhYeHq3LlyjLGKCoqSp07dy7ytvljb9iw\n",
       "oYYPH+484bBWrVoaP368atWqpVdffVXDhg2TdPaNesaMGed9MvpnKOx+HjNmjCIjI5WUlKS2bduq\n",
       "Q4cOeu+995yfLFSvXt25d6BmzZoaMmSI+vTpo2rVqqlKlSoaN27cBZfn27dvn7p06aLTp09rzJgx\n",
       "f9rJSqdPn1ZUVJQkOQ8fiY2N1QMPPKBnn31WMTExcnd31zXXXKNWrVo5D/n44/1U2P32j3/8QyNH\n",
       "jtT8+fPVoEED3XHHHZKk8PBwff755woLC1PVqlXVunVrHTlyRKdOnbrgp0P5qlWrpltvvVW5ubkF\n",
       "TuJ/4IEH9Prrrzt3g1euXFlvvvmmxo4dq7i4OOXl5empp55Sq1atCpxw+EcvvPCCRo0apbCwMHl6\n",
       "esrHx+ei43nkkUfUo0eP8z71z9eoUSOFhISoS5cuuvbaa3XNNdc4d2Xn32cPP/ywHA6HunXrJmOM\n",
       "6tev7/x6yku5ny/lMkkaMmSIJk2apKioKOdJxSNGjJCbm5tOnDihRx55xHldLy8vBQcH6/bbb1d4\n",
       "eLiqVq2q5cuXq3LlyoqNjdVTTz2lhg0bKjo6WsHBwdq2bZuks7v/PTw85OHhoW7duunpp5+Wq6ur\n",
       "XF1d1bx5c6WlpV1W6Fzq697lvHbm/3znnXeqUqVK6t69uxYuXFjgOp6enoqPj9fUqVP19ttvq0qV\n",
       "Kqpatap69uypbt26Xfb4ztW/f3+99NJLzhOymzZt6vww5EJjfeONN3T06FHNnj3b+a1tderU0auv\n",
       "vqqVK1cqPDxcVapUUevWrXX8+PECe27z3XfffUpMTNTDDz+sY8eO6Z577lG9evWUlJQkY4wGDhyo\n",
       "++67T6tWrSrz95tL9cdxTpw4UV26dNEbb7zhXNa5c2d9/vnnmjhxojOG/rj8pZdeUlBQkB555BG9\n",
       "9dZbzsMby4tz5xkQEKBPPvlEHTt2lJubm3x9fVWjRg3t3btXffv21fPPP6+IiAhVqVJFTZo0UUhI\n",
       "iDw9PS/4mCsp5em1OSEhwblHIDs7W/Xr19ff//53SdKwYcOu6H3tiSee0OrVqzVr1iznHrnywMVU\n",
       "pIwHUKhzv4UJAADgcnDIEHAVqCifxgEAgPKHPQQAAACAxdhDgP/ffh0IAAAAAAjytx7ksggAgDEh\n",
       "AACAMSEAAIAxIQAAgDEhAACAMSEAAICxACo8e+YEecdKAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1037aa890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for funders\n",
    "functional = df_new[df_new['status_group']=='functional']['funder'].tolist()\n",
    "non_functional = df_new[df_new['status_group']=='non functional']['funder'].tolist()\n",
    "needs_repair = df_new[df_new['status_group']=='functional needs repair']['funder'].tolist()\n",
    "\n",
    "plt.rc('xtick', labelsize=12) \n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "categories = pd.Series(df_new.funder.values.ravel()).unique()\n",
    "value_freq_funct = []\n",
    "value_freq_nonf = []\n",
    "value_freq_repair = []\n",
    "categories_num = range(1,len(categories)+1)\n",
    "for value in categories:\n",
    "    value_freq_funct.append(functional.count(value))\n",
    "    value_freq_nonf.append(non_functional.count(value))\n",
    "    value_freq_repair.append(needs_repair.count(value))\n",
    "\n",
    "plt.bar(categories_num,value_freq_funct,align='center',color='g')\n",
    "plt.bar(categories_num,value_freq_nonf,align='center',color='r')\n",
    "plt.bar(categories_num,value_freq_repair,align='center',color='b')\n",
    "plt.xticks(categories_num, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Other', 'Unicef', 'Rwssp', 'Danida', 'World Vision', 'Hesawa',\n",
       "       'Government Of Tanzania', nan, 'Kkkt', 'Tasaf', 'World Bank'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df.funder.values.ravel()).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are working with a lot of categorical feature data, we first need to turn the multiclass feature variables into indicators for each of the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>population</th>\n",
       "      <th>date_recorded_offset_days</th>\n",
       "      <th>status_group</th>\n",
       "      <th>funder_Danida</th>\n",
       "      <th>funder_Government Of Tanzania</th>\n",
       "      <th>funder_Hesawa</th>\n",
       "      <th>funder_Kkkt</th>\n",
       "      <th>funder_Other</th>\n",
       "      <th>...</th>\n",
       "      <th>date_recorded_month_Dec</th>\n",
       "      <th>date_recorded_month_Feb</th>\n",
       "      <th>date_recorded_month_Jan</th>\n",
       "      <th>date_recorded_month_Jul</th>\n",
       "      <th>date_recorded_month_Jun</th>\n",
       "      <th>date_recorded_month_Mar</th>\n",
       "      <th>date_recorded_month_May</th>\n",
       "      <th>date_recorded_month_Nov</th>\n",
       "      <th>date_recorded_month_Oct</th>\n",
       "      <th>date_recorded_month_Sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.831495</td>\n",
       "      <td>1.042939</td>\n",
       "      <td>-0.148715</td>\n",
       "      <td>1.135927</td>\n",
       "      <td>functional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>1.055942</td>\n",
       "      <td>0.205933</td>\n",
       "      <td>-1.021636</td>\n",
       "      <td>functional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.093236</td>\n",
       "      <td>0.025834</td>\n",
       "      <td>0.143714</td>\n",
       "      <td>-0.994778</td>\n",
       "      <td>functional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>-0.585296</td>\n",
       "      <td>-0.254488</td>\n",
       "      <td>-0.911221</td>\n",
       "      <td>non functional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101290</td>\n",
       "      <td>-0.965266</td>\n",
       "      <td>-0.374778</td>\n",
       "      <td>0.774841</td>\n",
       "      <td>functional</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount_tsh  gps_height  population  date_recorded_offset_days  \\\n",
       "0    1.831495    1.042939   -0.148715                   1.135927   \n",
       "1   -0.101290    1.055942    0.205933                  -1.021636   \n",
       "2   -0.093236    0.025834    0.143714                  -0.994778   \n",
       "3   -0.101290   -0.585296   -0.254488                  -0.911221   \n",
       "4   -0.101290   -0.965266   -0.374778                   0.774841   \n",
       "\n",
       "     status_group  funder_Danida  funder_Government Of Tanzania  \\\n",
       "0      functional              0                              0   \n",
       "1      functional              0                              0   \n",
       "2      functional              0                              0   \n",
       "3  non functional              0                              0   \n",
       "4      functional              0                              0   \n",
       "\n",
       "   funder_Hesawa  funder_Kkkt  funder_Other           ...             \\\n",
       "0              0            0             1           ...              \n",
       "1              0            0             1           ...              \n",
       "2              0            0             1           ...              \n",
       "3              0            0             0           ...              \n",
       "4              0            0             1           ...              \n",
       "\n",
       "   date_recorded_month_Dec  date_recorded_month_Feb  date_recorded_month_Jan  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        1                        0   \n",
       "3                        0                        0                        1   \n",
       "4                        0                        0                        0   \n",
       "\n",
       "   date_recorded_month_Jul  date_recorded_month_Jun  date_recorded_month_Mar  \\\n",
       "0                        0                        0                        1   \n",
       "1                        0                        0                        1   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        1                        0                        0   \n",
       "\n",
       "   date_recorded_month_May  date_recorded_month_Nov  date_recorded_month_Oct  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        0                        0                        0   \n",
       "\n",
       "   date_recorded_month_Sep  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_formatted includes all the data from df_new, with the categorical variables as indicators\n",
    "df_formatted = pd.get_dummies(df_new, columns=CATEGORICAL)\n",
    "df_formatted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feat_train and lab_train analogous to Xtrain and ytrain\n",
    "feat_train = df_formatted[mask].drop('status_group',axis=1)\n",
    "lab_train = pd.DataFrame(df_formatted[mask].status_group)\n",
    "##feat_test and lab_test analogous to Xtrain and ytrain\n",
    "feat_test = df_formatted[~mask].drop('status_group',axis=1)\n",
    "lab_test = pd.DataFrame(df_formatted[~mask].status_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Naive Bayes\n",
    "\n",
    "We begin with Naive Bayes to set up a baseline classifier. While the probabilities may not be too well calibrated, the relative order of of the probabilities across the labels is typically correct.\n",
    "\n",
    "Because we have a combination of categorical and continuous predictors, we first independently fit a Gaussian Naive Bayes model on the continuous data and a bernoulli Naive Bayes model on the categorical part. \n",
    "\n",
    "####TO DECIDE\n",
    "\n",
    "Then we __________ transform the entire dataset by taking the class assignment probabilities (with the predict_proba() method) as the new features: np.hstack((multinomial_probas, gaussian_probas)) and then refit a new model (e.g. a new gaussian NB) on these new features.\n",
    "\n",
    "calibrate (show plot)\n",
    "log -> sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "#separate categorical and continuous\n",
    "cont_feat_train = feat_train[STANDARDIZABLE].values\n",
    "cat_feat_train = feat_train.drop(STANDARDIZABLE,axis=1).values\n",
    "cont_feat_test = feat_test[STANDARDIZABLE].values\n",
    "cat_feat_test = feat_test.drop(STANDARDIZABLE,axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.425869809203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1025,   71,  319],\n",
       "       [   0,    0,    0],\n",
       "       [8617, 1224, 6564]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the continuous variables to a Gaussian Naive Bayes\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_clf.fit(cont_feat_train, lab_train)\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == gnb_clf.predict(cont_feat_test)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(gnb_clf.predict(cont_feat_test),lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.655274971942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7042,  545, 2125],\n",
       "       [ 955,  471,  594],\n",
       "       [1645,  279, 4164]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the categorical variables to a Bernoulli Naive Bayes\n",
    "bnb_clf = BernoulliNB()\n",
    "bnb_clf.fit(cat_feat_train, lab_train)\n",
    "\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == bnb_clf.predict(cat_feat_test)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(bnb_clf.predict(cat_feat_test),lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.677665544332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7590,  647, 2425],\n",
       "       [ 640,  384,  356],\n",
       "       [1412,  264, 4102]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the categorical variables to a Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(cat_feat_train, lab_train)\n",
    "\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == mnb_clf.predict(cat_feat_test)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(mnb_clf.predict(cat_feat_test),lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes does not work well with highly correlated features because the conditional independence assumption of Naive Bayes overinflates the importance of these correlated features by in effect counting them twice. Thus, we remove some of the redundant features, particularly the more broad feature in the nesting structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_df_new = df_new.drop(['management_group','source_class','extraction_type_class'], axis=1)\n",
    "small_df_formatted = pd.get_dummies(small_df_new, columns=list(set(CATEGORICAL).difference(['management_group','source_class','extraction_type_class'])))\n",
    "\n",
    "\n",
    "reduced_feat_train = small_df_formatted[mask].drop('status_group',axis=1)\n",
    "reduced_feat_test = small_df_formatted[~mask].drop('status_group',axis=1)\n",
    "\n",
    "cont_feat_train1 = reduced_feat_train[STANDARDIZABLE].values\n",
    "cat_feat_train1 = reduced_feat_train.drop(STANDARDIZABLE,axis=1).values\n",
    "cont_feat_test1 = reduced_feat_test[STANDARDIZABLE].values\n",
    "cat_feat_test1 = reduced_feat_test.drop(STANDARDIZABLE,axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.425869809203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1025,   71,  319],\n",
       "       [   0,    0,    0],\n",
       "       [8617, 1224, 6564]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_clf1 = GaussianNB()\n",
    "gnb_clf1.fit(cont_feat_train1, lab_train)\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == gnb_clf1.predict(cont_feat_test1)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(gnb_clf1.predict(cont_feat_test1),lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.669977553311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7170,  568, 2172],\n",
       "       [ 730,  427,  369],\n",
       "       [1742,  300, 4342]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the categorical variables to a Bernoulli Naive Bayes\n",
    "bnb_clf1 = BernoulliNB()\n",
    "bnb_clf1.fit(cat_feat_train1, lab_train)\n",
    "\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == bnb_clf1.predict(cat_feat_test1)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(bnb_clf1.predict(cat_feat_test1),lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a higher accuracy, yet we can still do better. We know that the Bernoulli model factors into account the nonoccurence of the different features. As a result, it puts more weight on the features that have many values associated, not factoring in inherent dependence and mutually exclusive nature of these feature. For example, if a row has a 1 for funder World Bank, it will have a 0 for all the other funders, and the Bernoulli model will take into account the 1 and all the 0s, despite them being very dependent.\n",
    "\n",
    "We instead fit the model for categorical feature variables to a Multinomial Naive Bayes. The model performs better, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frac of correctly labeled points 0.688496071829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7673,  673, 2388],\n",
       "       [ 489,  328,  227],\n",
       "       [1480,  294, 4268]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the categorical variables to a Multinomial Naive Bayes\n",
    "mnb_clf1 = MultinomialNB()\n",
    "mnb_clf1.fit(cat_feat_train1, lab_train)\n",
    "\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == mnb_clf1.predict(cat_feat_test1)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(mnb_clf1.predict(cat_feat_test1),lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we split the categorical and quantitative features, we did not get to use all our features. To allow for that, we bucket our quantitative features to turn them into categorical ones. We bucket them how we deem appropriate according to the graphs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#turn quantitative variables to categorical ones\n",
    "df_allcat = small_df_new.copy()\n",
    "\n",
    "df_allcat.ix[df_allcat.amount_tsh>40, 'amount_tsh'] = \"High\"\n",
    "df_allcat.ix[(df_allcat.amount_tsh>10) & (df_new.amount_tsh<=40), 'amount_tsh'] = \"Medium\"\n",
    "df_allcat.ix[df_allcat.amount_tsh<=10, 'amount_tsh'] = \"Low\"\n",
    "\n",
    "df_allcat.ix[df_allcat.gps_height>0, 'gps_height'] = \"Above avg\"\n",
    "df_allcat.ix[df_allcat.gps_height<=0, 'gps_height'] = \"Below avg\"\n",
    "\n",
    "df_allcat.ix[df_allcat.date_recorded_offset_days>0, 'date_recorded_offset_days'] = \"Above avg\"\n",
    "df_allcat.ix[df_allcat.date_recorded_offset_days<=0, 'date_recorded_offset_days'] = \"Below avg\"\n",
    "\n",
    "df_allcat.ix[df_allcat.population>10, 'population'] = \"High\"\n",
    "df_allcat.ix[(df_allcat.population<10)&(df_allcat.population>0), 'population'] = \"Medium\"\n",
    "df_allcat.ix[df_allcat.population<=0, 'population'] = \"Low\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_allcat = df_allcat.drop('status_group',axis=1)\n",
    "df_allcat_formatted = pd.get_dummies(df_allcat)\n",
    "\n",
    "new_feat_train = df_allcat_formatted[mask]\n",
    "new_feat_test = df_allcat_formatted[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Frac of correctly labeled points 0.682940516274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7597,  668, 2392],\n",
       "       [ 493,  333,  251],\n",
       "       [1552,  294, 4240]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit all variables to a Multinomial Naive Bayes\n",
    "mnb_clf2 = MultinomialNB()\n",
    "mnb_clf2.fit(new_feat_train, lab_train)\n",
    "\n",
    "print \"Frac of correctly labeled points\",float((lab_test.values.transpose().tolist() == mnb_clf2.predict(new_feat_test)).sum())/lab_test.shape[0]\n",
    "confusion_matrix(mnb_clf2.predict(new_feat_test),lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plat a calibration plot for our full Multinomial NB model, as well as the Multinomial NB on the reduced list of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: make it work for ytest being categorical\n",
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical Fraction\")\n",
    "\n",
    "    \n",
    "    #the distribution of P(fresh)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    #calsum = cal['count'].sum()\n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    plt.xlabel(\"Classifier Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(mnb_clf1, cont_feat_test1, lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(mnb_clf2, new_feat_test, lab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17820, 139)\n",
      "[[ 0.29493591  0.07536548  0.62969861  0.75753708  0.06756535  0.17489756]\n",
      " [ 0.21525895  0.05865076  0.72609029  0.57245036  0.30492416  0.12262548]\n",
      " [ 0.19871927  0.07104323  0.73023751  0.70299508  0.03008168  0.26692324]\n",
      " ..., \n",
      " [ 0.26510723  0.07571938  0.65917339  0.99126605  0.00300007  0.00573388]\n",
      " [ 0.22146002  0.05497675  0.72356323  0.88664278  0.05290528  0.06045194]\n",
      " [ 0.22398112  0.05357364  0.72244524  0.71583435  0.02800989  0.25615576]]\n",
      "***\n",
      "[[ 0.75753708  0.06756535  0.17489756]\n",
      " [ 0.57245036  0.30492416  0.12262548]\n",
      " [ 0.70299508  0.03008168  0.26692324]\n",
      " ..., \n",
      " [ 0.99126605  0.00300007  0.00573388]\n",
      " [ 0.88664278  0.05290528  0.06045194]\n",
      " [ 0.71583435  0.02800989  0.25615576]]\n",
      "***\n",
      "(41580, 135)\n"
     ]
    }
   ],
   "source": [
    "# #TODO\n",
    "# #optimize with Fisher's method\n",
    "# #combine for mixed Bayes?\n",
    "\n",
    "# #find the probabilities for the Multinomial and Gaussian Naive Bayes\n",
    "# gnb_probs = gnb_clf1.predict_proba(cont_feat_train1)\n",
    "# mnb_probs = mnb_clf1.predict_proba(cat_feat_train1)\n",
    "\n",
    "# #combine\n",
    "# feat_probs = np.hstack((gnb_probs, mnb_probs))\n",
    "\n",
    "# nb_clf = GaussianNB()\n",
    "# #nb_clf.fit(feat_probs, lab_train)\n",
    "# feat_probs = nb_clf.predict_proba(feat_test)\n",
    "# print reduced_feat_test.shape\n",
    "# print feat_probs\n",
    "# print '***'\n",
    "# print mnb_probs\n",
    "# print '***'\n",
    "# print cat_feat_train1.shape\n",
    "# #don't know how to change the inputs to probabilities \n",
    "# # print \"Frac of mislabeled points\",float((lab_test.values.transpose().tolist() != nb_clf.predict(reduced_feat_test)).sum())/lab_test.shape[0]\n",
    "# # confusion_matrix(nb_clf.predict(reduced_feat_test),lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one problem with this above approach, however, is that .predict_proba(X) method will return posteriors normalized over all classes. What we want is the unnormalized probabilities so that we can multiply them together. However, if they are normalized differently (which they are, since they are normalized for the classes), the product of such probabilities is not very reliable.http://stats.stackexchange.com/questions/93928/naive-bayes-continuous-and-categorical-predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mixing the Gaussian and Multinomial models is rather difficult, we move to a model that can more easily include both: the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVM\n",
    "\n",
    "Add Cindy's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Optimizing the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write two functions: cv_optimize() use GridSearchCV to choose the best regularization hyperparameter for the model, and do_classify() to print out the model fit on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the score() function in sklearn.linear_model.LogisticRegression. Other model evaluation/comparison measures we could consider are the balanced F-score (sklearn.metrics.f1_score()), the Kappa Statistic, Confusion Matrix, Hamming Loss, and Zero One loss. More options can be found here: http://scikit-learn.org/stable/modules/model_evaluation.html. (#TODO if more time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=indf[targetname].values\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Regression\n",
    "\n",
    "Because we have a categorical outcome variable with more than one outcome, we conduct a multinomial logistic regression with LASSO (specifically, L2) regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualizations up above, with 1. the wells shown in their respective geographic locations and 2. the proportion of functional/non functional/functional-needs-repair different across regions and subvillages, we sense that there may be some sort of clustering of wells by subvillage and region. Thus, we first create a multilevel model with two levels, one for the individual wells and the second for the subvillages. This creates a different correlation structure between the data points based on what group they are in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##classifier without optimizing for C\n",
    "mlr_clf = LogisticRegression(penalty=\"l2\",solver='newton-cg', multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "mlr_clf = mlr_clf.fit(feat_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.74\n",
      "Accuracy on test data:     0.73\n",
      "[[8592   76 1045]\n",
      " [ 932   96  245]\n",
      " [2451   45 4338]]\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = mlr_clf.score(feat_train, lab_train)\n",
    "test_accuracy = mlr_clf.score(feat_test, lab_test)\n",
    "print \"############# based on standard predict ################\"\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "print confusion_matrix(lab_test, mlr_clf.predict(feat_test))\n",
    "print \"########################################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the fitted model itself and interpret the coefficients. #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.20513196, -1.51050906,  0.3053771 ])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr_clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.73\n",
      "Accuracy on test data:     0.74\n",
      "[[8616   52  958]\n",
      " [ 950  107  236]\n",
      " [2437   45 4419]]\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "mlr_clf2 = LogisticRegression(penalty=\"l2\",solver='newton-cg', multi_class='multinomial',C=1000)\n",
    "mlr_clf2 = mlr_clf2.fit(feat_train, lab_train)\n",
    "training_accuracy = mlr_clf2.score(feat_train, lab_train)\n",
    "test_accuracy = mlr_clf2.score(feat_test, lab_test)\n",
    "print \"############# based on standard predict ################\"\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "print confusion_matrix(lab_test, mlr_clf2.predict(feat_test))\n",
    "print \"########################################################\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# -Mixed Naive Bayes\n",
    "# -interpret regression coefficients\n",
    "# -MLM\n",
    "# -Thresholdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 2 µs, total: 7 µs\n",
      "Wall time: 25 µs\n",
      "using mask\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-038a41095aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m clfmlr, Xtrain, ytrain, Xtest, ytest = do_classify(LogisticRegression(penalty=\"l2\",solver='newton-cg', multi_class='multinomial'), \n\u001b[0;32m----> 5\u001b[0;31m                                                    {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, df_formatted,feat_cols, u'status_group',1, mask=mask)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-504b9e99545b>\u001b[0m in \u001b[0;36mdo_classify\u001b[0;34m(clf, parameters, indf, featurenames, targetname, target1val, mask, reuse_split, score_func, n_folds, n_jobs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Xtrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Xtest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ytrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ytest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-21d7d7f25915>\u001b[0m in \u001b[0;36mcv_optimize\u001b[0;34m(clf, parameters, X, y, n_jobs, n_folds, score_func)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"BEST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 for train, test in cv)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m                 class_weight=self.class_weight)\n\u001b[0m\u001b[1;32m   1065\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             w0 = newton_cg(hess, func, grad, w0, args=args, maxiter=max_iter,\n\u001b[0;32m--> 660\u001b[0;31m                            tol=tol)\n\u001b[0m\u001b[1;32m    661\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             coef_, intercept_, _, = _fit_liblinear(\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/utils/optimize.pyc\u001b[0m in \u001b[0;36mnewton_cg\u001b[0;34m(func_grad_hess, func, grad, x0, args, eps, tol, maxiter)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# avoid inverting the Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtermcond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mAp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhess_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;31m# check curvature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcurv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mhessProd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mhessProd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_yhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mhessProd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feat_cols=list(df_formatted.columns)\n",
    "feat_cols.remove(u'status_group')\n",
    "%time\n",
    "clfmlr, Xtrain, ytrain, Xtest, ytest = do_classify(LogisticRegression(penalty=\"l2\",solver='newton-cg', multi_class='multinomial'), \n",
    "                                                   {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, df_formatted,feat_cols, u'status_group',1, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run a Random Forest, optimizing for the number of trees and the number of features to consider for the best split each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:5: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST {'n_estimators': 19} 0.775157654197 [mean: 0.72833, std: 0.00083, params: {'n_estimators': 1}, mean: 0.74226, std: 0.00480, params: {'n_estimators': 2}, mean: 0.75475, std: 0.00339, params: {'n_estimators': 3}, mean: 0.75988, std: 0.00219, params: {'n_estimators': 4}, mean: 0.76246, std: 0.00271, params: {'n_estimators': 5}, mean: 0.76741, std: 0.00355, params: {'n_estimators': 6}, mean: 0.76726, std: 0.00414, params: {'n_estimators': 7}, mean: 0.76844, std: 0.00236, params: {'n_estimators': 8}, mean: 0.76959, std: 0.00292, params: {'n_estimators': 9}, mean: 0.77053, std: 0.00252, params: {'n_estimators': 10}, mean: 0.77259, std: 0.00269, params: {'n_estimators': 11}, mean: 0.77458, std: 0.00554, params: {'n_estimators': 12}, mean: 0.77424, std: 0.00350, params: {'n_estimators': 13}, mean: 0.77482, std: 0.00265, params: {'n_estimators': 14}, mean: 0.77111, std: 0.00265, params: {'n_estimators': 15}, mean: 0.77495, std: 0.00308, params: {'n_estimators': 16}, mean: 0.77211, std: 0.00314, params: {'n_estimators': 17}, mean: 0.77465, std: 0.00497, params: {'n_estimators': 18}, mean: 0.77516, std: 0.00206, params: {'n_estimators': 19}]\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.96\n",
      "Accuracy on test data:     0.78\n",
      "[[8245  345 1052]\n",
      " [ 637  456  202]\n",
      " [1504  149 5230]]\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/lilyzhang/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfForest = RandomForestClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(1, 20)}\n",
    "clfForest, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest, parameters, \n",
    "                                                       df_formatted, feat_cols, 'status_group', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of trees (n_estimators)\n",
    "len(clfForest.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is at the maximum of our range. Thus, we increase the the range that we optimize over during our GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "BEST {'n_estimators': 35} 0.778466619244 [mean: 0.77501, std: 0.00407, params: {'n_estimators': 18}, mean: 0.77566, std: 0.00340, params: {'n_estimators': 19}, mean: 0.77478, std: 0.00217, params: {'n_estimators': 20}, mean: 0.77414, std: 0.00359, params: {'n_estimators': 21}, mean: 0.77384, std: 0.00377, params: {'n_estimators': 22}, mean: 0.77621, std: 0.00379, params: {'n_estimators': 23}, mean: 0.77677, std: 0.00243, params: {'n_estimators': 24}, mean: 0.77611, std: 0.00332, params: {'n_estimators': 25}, mean: 0.77626, std: 0.00266, params: {'n_estimators': 26}, mean: 0.77535, std: 0.00259, params: {'n_estimators': 27}, mean: 0.77447, std: 0.00437, params: {'n_estimators': 28}, mean: 0.77710, std: 0.00270, params: {'n_estimators': 29}, mean: 0.77541, std: 0.00309, params: {'n_estimators': 30}, mean: 0.77555, std: 0.00386, params: {'n_estimators': 31}, mean: 0.77681, std: 0.00338, params: {'n_estimators': 32}, mean: 0.77601, std: 0.00288, params: {'n_estimators': 33}, mean: 0.77682, std: 0.00241, params: {'n_estimators': 34}, mean: 0.77847, std: 0.00437, params: {'n_estimators': 35}, mean: 0.77714, std: 0.00389, params: {'n_estimators': 36}, mean: 0.77692, std: 0.00216, params: {'n_estimators': 37}, mean: 0.77659, std: 0.00204, params: {'n_estimators': 38}, mean: 0.77637, std: 0.00329, params: {'n_estimators': 39}]\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.97\n",
      "Accuracy on test data:     0.79\n",
      "[[8251  351 1040]\n",
      " [ 634  444  217]\n",
      " [1433  149 5301]]\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "clfForest1 = RandomForestClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(18, 40)}\n",
    "clfForest1, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest1, parameters, \n",
    "                                                       df_formatted, feat_cols, 'status_group', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that we have not hit the boundary of the range we gave and indeed found an optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clfForest1.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above classifier, we set the max_depth to None, meaning nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples (in this case, 2). We try limiting the max depth by increasing the min_samples_split size to see if this improves our performance, especially since it seems like the above classifier is overfitting to the training set. Thus we add min_sample_split to parameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "BEST {'min_samples_split': 7, 'n_estimators': 35} 0.785534530765 [mean: 0.77548, std: 0.00265, params: {'min_samples_split': 2, 'n_estimators': 18}, mean: 0.77365, std: 0.00286, params: {'min_samples_split': 2, 'n_estimators': 19}, mean: 0.77474, std: 0.00464, params: {'min_samples_split': 2, 'n_estimators': 20}, mean: 0.77358, std: 0.00425, params: {'min_samples_split': 2, 'n_estimators': 21}, mean: 0.77479, std: 0.00304, params: {'min_samples_split': 2, 'n_estimators': 22}, mean: 0.77505, std: 0.00263, params: {'min_samples_split': 2, 'n_estimators': 23}, mean: 0.77447, std: 0.00370, params: {'min_samples_split': 2, 'n_estimators': 24}, mean: 0.77594, std: 0.00305, params: {'min_samples_split': 2, 'n_estimators': 25}, mean: 0.77633, std: 0.00371, params: {'min_samples_split': 2, 'n_estimators': 26}, mean: 0.77552, std: 0.00422, params: {'min_samples_split': 2, 'n_estimators': 27}, mean: 0.77548, std: 0.00369, params: {'min_samples_split': 2, 'n_estimators': 28}, mean: 0.77557, std: 0.00266, params: {'min_samples_split': 2, 'n_estimators': 29}, mean: 0.77677, std: 0.00378, params: {'min_samples_split': 2, 'n_estimators': 30}, mean: 0.77601, std: 0.00261, params: {'min_samples_split': 2, 'n_estimators': 31}, mean: 0.77708, std: 0.00353, params: {'min_samples_split': 2, 'n_estimators': 32}, mean: 0.77655, std: 0.00232, params: {'min_samples_split': 2, 'n_estimators': 33}, mean: 0.77722, std: 0.00287, params: {'min_samples_split': 2, 'n_estimators': 34}, mean: 0.77656, std: 0.00321, params: {'min_samples_split': 2, 'n_estimators': 35}, mean: 0.77633, std: 0.00327, params: {'min_samples_split': 2, 'n_estimators': 36}, mean: 0.77681, std: 0.00388, params: {'min_samples_split': 2, 'n_estimators': 37}, mean: 0.77690, std: 0.00273, params: {'min_samples_split': 2, 'n_estimators': 38}, mean: 0.77786, std: 0.00449, params: {'min_samples_split': 2, 'n_estimators': 39}, mean: 0.77847, std: 0.00224, params: {'min_samples_split': 3, 'n_estimators': 18}, mean: 0.78153, std: 0.00221, params: {'min_samples_split': 3, 'n_estimators': 19}, mean: 0.77748, std: 0.00338, params: {'min_samples_split': 3, 'n_estimators': 20}, mean: 0.77883, std: 0.00287, params: {'min_samples_split': 3, 'n_estimators': 21}, mean: 0.77940, std: 0.00229, params: {'min_samples_split': 3, 'n_estimators': 22}, mean: 0.77764, std: 0.00218, params: {'min_samples_split': 3, 'n_estimators': 23}, mean: 0.78009, std: 0.00365, params: {'min_samples_split': 3, 'n_estimators': 24}, mean: 0.78192, std: 0.00170, params: {'min_samples_split': 3, 'n_estimators': 25}, mean: 0.77875, std: 0.00273, params: {'min_samples_split': 3, 'n_estimators': 26}, mean: 0.77836, std: 0.00421, params: {'min_samples_split': 3, 'n_estimators': 27}, mean: 0.78001, std: 0.00317, params: {'min_samples_split': 3, 'n_estimators': 28}, mean: 0.77958, std: 0.00307, params: {'min_samples_split': 3, 'n_estimators': 29}, mean: 0.78043, std: 0.00451, params: {'min_samples_split': 3, 'n_estimators': 30}, mean: 0.77943, std: 0.00343, params: {'min_samples_split': 3, 'n_estimators': 31}, mean: 0.77930, std: 0.00239, params: {'min_samples_split': 3, 'n_estimators': 32}, mean: 0.78164, std: 0.00292, params: {'min_samples_split': 3, 'n_estimators': 33}, mean: 0.78098, std: 0.00237, params: {'min_samples_split': 3, 'n_estimators': 34}, mean: 0.77970, std: 0.00186, params: {'min_samples_split': 3, 'n_estimators': 35}, mean: 0.78065, std: 0.00200, params: {'min_samples_split': 3, 'n_estimators': 36}, mean: 0.77943, std: 0.00379, params: {'min_samples_split': 3, 'n_estimators': 37}, mean: 0.78122, std: 0.00357, params: {'min_samples_split': 3, 'n_estimators': 38}, mean: 0.77940, std: 0.00290, params: {'min_samples_split': 3, 'n_estimators': 39}, mean: 0.78045, std: 0.00387, params: {'min_samples_split': 4, 'n_estimators': 18}, mean: 0.78069, std: 0.00174, params: {'min_samples_split': 4, 'n_estimators': 19}, mean: 0.78184, std: 0.00161, params: {'min_samples_split': 4, 'n_estimators': 20}, mean: 0.78125, std: 0.00319, params: {'min_samples_split': 4, 'n_estimators': 21}, mean: 0.78084, std: 0.00281, params: {'min_samples_split': 4, 'n_estimators': 22}, mean: 0.78209, std: 0.00337, params: {'min_samples_split': 4, 'n_estimators': 23}, mean: 0.78110, std: 0.00225, params: {'min_samples_split': 4, 'n_estimators': 24}, mean: 0.78145, std: 0.00233, params: {'min_samples_split': 4, 'n_estimators': 25}, mean: 0.78144, std: 0.00361, params: {'min_samples_split': 4, 'n_estimators': 26}, mean: 0.78172, std: 0.00332, params: {'min_samples_split': 4, 'n_estimators': 27}, mean: 0.78256, std: 0.00205, params: {'min_samples_split': 4, 'n_estimators': 28}, mean: 0.78157, std: 0.00268, params: {'min_samples_split': 4, 'n_estimators': 29}, mean: 0.78136, std: 0.00153, params: {'min_samples_split': 4, 'n_estimators': 30}, mean: 0.78312, std: 0.00221, params: {'min_samples_split': 4, 'n_estimators': 31}, mean: 0.78290, std: 0.00272, params: {'min_samples_split': 4, 'n_estimators': 32}, mean: 0.78270, std: 0.00416, params: {'min_samples_split': 4, 'n_estimators': 33}, mean: 0.78308, std: 0.00192, params: {'min_samples_split': 4, 'n_estimators': 34}, mean: 0.78123, std: 0.00365, params: {'min_samples_split': 4, 'n_estimators': 35}, mean: 0.78208, std: 0.00322, params: {'min_samples_split': 4, 'n_estimators': 36}, mean: 0.78373, std: 0.00315, params: {'min_samples_split': 4, 'n_estimators': 37}, mean: 0.78339, std: 0.00300, params: {'min_samples_split': 4, 'n_estimators': 38}, mean: 0.78309, std: 0.00191, params: {'min_samples_split': 4, 'n_estimators': 39}, mean: 0.78089, std: 0.00336, params: {'min_samples_split': 5, 'n_estimators': 18}, mean: 0.78036, std: 0.00349, params: {'min_samples_split': 5, 'n_estimators': 19}, mean: 0.78171, std: 0.00266, params: {'min_samples_split': 5, 'n_estimators': 20}, mean: 0.78136, std: 0.00146, params: {'min_samples_split': 5, 'n_estimators': 21}, mean: 0.78300, std: 0.00462, params: {'min_samples_split': 5, 'n_estimators': 22}, mean: 0.78309, std: 0.00240, params: {'min_samples_split': 5, 'n_estimators': 23}, mean: 0.78233, std: 0.00230, params: {'min_samples_split': 5, 'n_estimators': 24}, mean: 0.78070, std: 0.00113, params: {'min_samples_split': 5, 'n_estimators': 25}, mean: 0.78153, std: 0.00296, params: {'min_samples_split': 5, 'n_estimators': 26}, mean: 0.78019, std: 0.00264, params: {'min_samples_split': 5, 'n_estimators': 27}, mean: 0.78200, std: 0.00275, params: {'min_samples_split': 5, 'n_estimators': 28}, mean: 0.78297, std: 0.00427, params: {'min_samples_split': 5, 'n_estimators': 29}, mean: 0.78346, std: 0.00121, params: {'min_samples_split': 5, 'n_estimators': 30}, mean: 0.78393, std: 0.00276, params: {'min_samples_split': 5, 'n_estimators': 31}, mean: 0.78302, std: 0.00322, params: {'min_samples_split': 5, 'n_estimators': 32}, mean: 0.78314, std: 0.00202, params: {'min_samples_split': 5, 'n_estimators': 33}, mean: 0.78205, std: 0.00152, params: {'min_samples_split': 5, 'n_estimators': 34}, mean: 0.78420, std: 0.00276, params: {'min_samples_split': 5, 'n_estimators': 35}, mean: 0.78513, std: 0.00214, params: {'min_samples_split': 5, 'n_estimators': 36}, mean: 0.78398, std: 0.00113, params: {'min_samples_split': 5, 'n_estimators': 37}, mean: 0.78416, std: 0.00215, params: {'min_samples_split': 5, 'n_estimators': 38}, mean: 0.78419, std: 0.00222, params: {'min_samples_split': 5, 'n_estimators': 39}, mean: 0.77972, std: 0.00188, params: {'min_samples_split': 6, 'n_estimators': 18}, mean: 0.78123, std: 0.00303, params: {'min_samples_split': 6, 'n_estimators': 19}, mean: 0.78183, std: 0.00202, params: {'min_samples_split': 6, 'n_estimators': 20}, mean: 0.78233, std: 0.00167, params: {'min_samples_split': 6, 'n_estimators': 21}, mean: 0.78286, std: 0.00216, params: {'min_samples_split': 6, 'n_estimators': 22}, mean: 0.78390, std: 0.00195, params: {'min_samples_split': 6, 'n_estimators': 23}, mean: 0.78385, std: 0.00183, params: {'min_samples_split': 6, 'n_estimators': 24}, mean: 0.78396, std: 0.00180, params: {'min_samples_split': 6, 'n_estimators': 25}, mean: 0.78375, std: 0.00261, params: {'min_samples_split': 6, 'n_estimators': 26}, mean: 0.78313, std: 0.00193, params: {'min_samples_split': 6, 'n_estimators': 27}, mean: 0.78177, std: 0.00255, params: {'min_samples_split': 6, 'n_estimators': 28}, mean: 0.78508, std: 0.00232, params: {'min_samples_split': 6, 'n_estimators': 29}, mean: 0.78405, std: 0.00119, params: {'min_samples_split': 6, 'n_estimators': 30}, mean: 0.78258, std: 0.00242, params: {'min_samples_split': 6, 'n_estimators': 31}, mean: 0.78353, std: 0.00136, params: {'min_samples_split': 6, 'n_estimators': 32}, mean: 0.78361, std: 0.00255, params: {'min_samples_split': 6, 'n_estimators': 33}, mean: 0.78477, std: 0.00149, params: {'min_samples_split': 6, 'n_estimators': 34}, mean: 0.78196, std: 0.00198, params: {'min_samples_split': 6, 'n_estimators': 35}, mean: 0.78270, std: 0.00122, params: {'min_samples_split': 6, 'n_estimators': 36}, mean: 0.78390, std: 0.00253, params: {'min_samples_split': 6, 'n_estimators': 37}, mean: 0.78450, std: 0.00211, params: {'min_samples_split': 6, 'n_estimators': 38}, mean: 0.78498, std: 0.00188, params: {'min_samples_split': 6, 'n_estimators': 39}, mean: 0.78100, std: 0.00255, params: {'min_samples_split': 7, 'n_estimators': 18}, mean: 0.78136, std: 0.00198, params: {'min_samples_split': 7, 'n_estimators': 19}, mean: 0.78209, std: 0.00257, params: {'min_samples_split': 7, 'n_estimators': 20}, mean: 0.78198, std: 0.00264, params: {'min_samples_split': 7, 'n_estimators': 21}, mean: 0.78274, std: 0.00303, params: {'min_samples_split': 7, 'n_estimators': 22}, mean: 0.78333, std: 0.00164, params: {'min_samples_split': 7, 'n_estimators': 23}, mean: 0.78457, std: 0.00241, params: {'min_samples_split': 7, 'n_estimators': 24}, mean: 0.78303, std: 0.00302, params: {'min_samples_split': 7, 'n_estimators': 25}, mean: 0.78224, std: 0.00294, params: {'min_samples_split': 7, 'n_estimators': 26}, mean: 0.78312, std: 0.00294, params: {'min_samples_split': 7, 'n_estimators': 27}, mean: 0.78196, std: 0.00227, params: {'min_samples_split': 7, 'n_estimators': 28}, mean: 0.78391, std: 0.00256, params: {'min_samples_split': 7, 'n_estimators': 29}, mean: 0.78355, std: 0.00175, params: {'min_samples_split': 7, 'n_estimators': 30}, mean: 0.78547, std: 0.00198, params: {'min_samples_split': 7, 'n_estimators': 31}, mean: 0.78436, std: 0.00196, params: {'min_samples_split': 7, 'n_estimators': 32}, mean: 0.78437, std: 0.00364, params: {'min_samples_split': 7, 'n_estimators': 33}, mean: 0.78316, std: 0.00241, params: {'min_samples_split': 7, 'n_estimators': 34}, mean: 0.78553, std: 0.00141, params: {'min_samples_split': 7, 'n_estimators': 35}, mean: 0.78406, std: 0.00141, params: {'min_samples_split': 7, 'n_estimators': 36}, mean: 0.78426, std: 0.00245, params: {'min_samples_split': 7, 'n_estimators': 37}, mean: 0.78329, std: 0.00248, params: {'min_samples_split': 7, 'n_estimators': 38}, mean: 0.78362, std: 0.00130, params: {'min_samples_split': 7, 'n_estimators': 39}, mean: 0.78031, std: 0.00327, params: {'min_samples_split': 8, 'n_estimators': 18}, mean: 0.78178, std: 0.00224, params: {'min_samples_split': 8, 'n_estimators': 19}, mean: 0.78204, std: 0.00256, params: {'min_samples_split': 8, 'n_estimators': 20}, mean: 0.78134, std: 0.00386, params: {'min_samples_split': 8, 'n_estimators': 21}, mean: 0.78250, std: 0.00247, params: {'min_samples_split': 8, 'n_estimators': 22}, mean: 0.78260, std: 0.00213, params: {'min_samples_split': 8, 'n_estimators': 23}, mean: 0.78299, std: 0.00227, params: {'min_samples_split': 8, 'n_estimators': 24}, mean: 0.78135, std: 0.00195, params: {'min_samples_split': 8, 'n_estimators': 25}, mean: 0.78336, std: 0.00202, params: {'min_samples_split': 8, 'n_estimators': 26}, mean: 0.78335, std: 0.00332, params: {'min_samples_split': 8, 'n_estimators': 27}, mean: 0.78275, std: 0.00220, params: {'min_samples_split': 8, 'n_estimators': 28}, mean: 0.78177, std: 0.00222, params: {'min_samples_split': 8, 'n_estimators': 29}, mean: 0.78263, std: 0.00200, params: {'min_samples_split': 8, 'n_estimators': 30}, mean: 0.78299, std: 0.00180, params: {'min_samples_split': 8, 'n_estimators': 31}, mean: 0.78415, std: 0.00203, params: {'min_samples_split': 8, 'n_estimators': 32}, mean: 0.78312, std: 0.00245, params: {'min_samples_split': 8, 'n_estimators': 33}, mean: 0.78339, std: 0.00359, params: {'min_samples_split': 8, 'n_estimators': 34}, mean: 0.78326, std: 0.00283, params: {'min_samples_split': 8, 'n_estimators': 35}, mean: 0.78326, std: 0.00328, params: {'min_samples_split': 8, 'n_estimators': 36}, mean: 0.78296, std: 0.00263, params: {'min_samples_split': 8, 'n_estimators': 37}, mean: 0.78371, std: 0.00323, params: {'min_samples_split': 8, 'n_estimators': 38}, mean: 0.78345, std: 0.00166, params: {'min_samples_split': 8, 'n_estimators': 39}, mean: 0.78019, std: 0.00367, params: {'min_samples_split': 9, 'n_estimators': 18}, mean: 0.78024, std: 0.00353, params: {'min_samples_split': 9, 'n_estimators': 19}, mean: 0.78181, std: 0.00177, params: {'min_samples_split': 9, 'n_estimators': 20}, mean: 0.78241, std: 0.00232, params: {'min_samples_split': 9, 'n_estimators': 21}, mean: 0.78196, std: 0.00244, params: {'min_samples_split': 9, 'n_estimators': 22}, mean: 0.78200, std: 0.00153, params: {'min_samples_split': 9, 'n_estimators': 23}, mean: 0.78349, std: 0.00260, params: {'min_samples_split': 9, 'n_estimators': 24}, mean: 0.78044, std: 0.00350, params: {'min_samples_split': 9, 'n_estimators': 25}, mean: 0.78158, std: 0.00212, params: {'min_samples_split': 9, 'n_estimators': 26}, mean: 0.78257, std: 0.00240, params: {'min_samples_split': 9, 'n_estimators': 27}, mean: 0.78163, std: 0.00139, params: {'min_samples_split': 9, 'n_estimators': 28}, mean: 0.78338, std: 0.00261, params: {'min_samples_split': 9, 'n_estimators': 29}, mean: 0.78219, std: 0.00337, params: {'min_samples_split': 9, 'n_estimators': 30}, mean: 0.78236, std: 0.00228, params: {'min_samples_split': 9, 'n_estimators': 31}, mean: 0.78427, std: 0.00285, params: {'min_samples_split': 9, 'n_estimators': 32}, mean: 0.78262, std: 0.00148, params: {'min_samples_split': 9, 'n_estimators': 33}, mean: 0.78289, std: 0.00178, params: {'min_samples_split': 9, 'n_estimators': 34}, mean: 0.78157, std: 0.00192, params: {'min_samples_split': 9, 'n_estimators': 35}, mean: 0.78164, std: 0.00209, params: {'min_samples_split': 9, 'n_estimators': 36}, mean: 0.78254, std: 0.00241, params: {'min_samples_split': 9, 'n_estimators': 37}, mean: 0.78324, std: 0.00185, params: {'min_samples_split': 9, 'n_estimators': 38}, mean: 0.78305, std: 0.00199, params: {'min_samples_split': 9, 'n_estimators': 39}]\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.90\n",
      "Accuracy on test data:     0.80\n",
      "[[8566  210  866]\n",
      " [ 709  387  199]\n",
      " [1546   96 5241]]\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "clfForest2 = RandomForestClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(18,40), \"min_samples_split\": range(2,10)}\n",
    "clfForest2, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest2, parameters, \n",
    "                                                       df_formatted, feat_cols, 'status_group', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Thresholdout\n",
    "\n",
    "There is a problem with using and reusing the test set to estimate the accuracy of a model. Specifically, we run into problems of overfitting since we use the results of on test set to adapt the model over multiple iteration, so that when we fit to the test set the nth time, the model is no longer completely independent of the test set that helped refine it in the previous example. Based on the findings of a recent paper that came out in _Science_, \"The reusable holdout: Preserving validity in adaptive data analysis,\" we run a Thresholdout, where we add noise to the holdout set, or test set, every time we use it. Based on the relative accuracy measurements on the test and training set, we take either the training set's accuracy measurement of the test set's accuracy measurement plus random noise. We have a maximum number of times we can iterate based on the variance we allow in our measurements and the size of the holdout set. More information can be found here: http://arxiv.org/pdf/1506.02629v1.pdf and https://www.sciencemag.org/content/349/6248/636.short.\n",
    "\n",
    "We added this functionality to do_classify in the function do_classify2()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the following, we first install basemap with 'conda install basemap.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named basemap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-87ca1c6893c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# setup Lambert Conformal basemap.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m m = Basemap(projection='cyl',\n\u001b[1;32m      5\u001b[0m             \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named basemap"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# setup Lambert Conformal basemap.\n",
    "m = Basemap(projection='cyl',\n",
    "            resolution='h',\n",
    "            lat_0=-6,lon_0=34., \n",
    "            llcrnrlon=25, llcrnrlat=-12,\n",
    "            urcrnrlon=43, urcrnrlat=0)\n",
    "\n",
    "# draw coastlines.\n",
    "m.drawcoastlines()\n",
    "\n",
    "# draw countries borders.\n",
    "m.drawcountries()\n",
    "\n",
    "# draw a boundary around the map, fill the background.\n",
    "# this background will end up being the ocean color, since\n",
    "# the continents will be drawn on top.\n",
    "m.drawmapboundary(fill_color='aqua')\n",
    "\n",
    "# fill continents, set lake color same as ocean color.\n",
    "m.fillcontinents(color='white',lake_color='aqua', zorder = 0)\n",
    "\n",
    "# create scatter plot\n",
    "use_colors = {\"functional\": \"green\", \"non functional\": \"red\", \"functional needs repair\": \"blue\"}\n",
    "m.scatter(df_new.longitude.values, df_new.latitude.values, marker = 'o',zorder = 1, latlon=True, alpha = 0.05, \n",
    "          c=[use_colors[x] for x in df_new.status_group],s=30,lw=0)\n",
    "\n",
    "# create legend\n",
    "line1 = plt.Line2D(range(1), range(1), color=\"white\", marker='o',markerfacecolor=\"blue\", markersize = 6)\n",
    "line2 = plt.Line2D(range(1), range(1), color=\"white\", marker='o',markerfacecolor=\"green\", markersize = 6)\n",
    "line3 = plt.Line2D(range(1), range(1), color=\"white\", marker='o',markerfacecolor=\"red\", markersize = 6)\n",
    "plt.legend((line1,line2,line3),(\"functional need repair\",\"functional\",\"non functional\"),numpoints=1, loc=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
